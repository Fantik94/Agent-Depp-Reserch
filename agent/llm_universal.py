import requests
import json
import time
import logging
from typing import List, Dict, Optional
from config import Config

# Couleurs pour les logs
try:
    from colorama import init, Fore, Back, Style
    init(autoreset=True)
    COLORS_AVAILABLE = True
except ImportError:
    COLORS_AVAILABLE = False
    Fore = Back = Style = type('', (), {'__getattr__': lambda self, name: ''})()

logger = logging.getLogger(__name__)

def colored_log(level, message, color=None):
    """Log avec couleur si disponible"""
    if COLORS_AVAILABLE and color:
        print(f"{color}{message}{Style.RESET_ALL}")
    else:
        getattr(logger, level)(message)

class UniversalLLMClient:
    """Client LLM universel supportant Mistral, Groq et Ollama"""
    
    def __init__(self, provider: str = None):
        self.config = Config()
        self.provider = provider or self.config.LLM_PROVIDER
        self.last_request_time = 0
        self.min_delay = 1  # D√©lai r√©duit
        
        # Initialiser le client selon le provider
        if self.provider == "mistral":
            try:
                from mistralai import Mistral
                from mistralai.models import UserMessage
                self.client = Mistral(api_key=self.config.MISTRAL_API_KEY)
                self.UserMessage = UserMessage
                # Headers pour les requ√™tes directes
                self.mistral_headers = {
                    "Authorization": f"Bearer {self.config.MISTRAL_API_KEY}",
                    "Content-Type": "application/json"
                }
                logger.info(f"ü§ñ Client Mistral initialis√© avec {self.config.MISTRAL_MODEL}")
            except Exception as e:
                logger.error(f"‚ùå Erreur initialisation Mistral: {e}")
                logger.info("üîÑ Basculement vers Groq")
                self.provider = "groq"  # Fallback vers Groq
                self._init_groq()
        elif self.provider == "groq":
            self._init_groq()
        elif self.provider == "ollama":
            # Ollama local
            pass
        else:
            logger.warning(f"‚ö†Ô∏è Provider inconnu: {self.provider}, utilisation de Groq")
            self.provider = "groq"
            self._init_groq()
            
        logger.info(f"ü§ñ Client LLM initialis√©: {self.provider}")
    
    def _init_groq(self):
        """Initialise Groq avec les headers n√©cessaires"""
        if not self.config.GROQ_API_KEY:
            logger.error("‚ùå Cl√© API Groq manquante")
            return
        
        self.groq_headers = {
            "Authorization": f"Bearer {self.config.GROQ_API_KEY}",
            "Content-Type": "application/json"
        }
        logger.info("ü§ñ Headers Groq initialis√©s")
    
    def _wait_for_rate_limit(self):
        """Attend avant la prochaine requ√™te (d√©lai r√©duit)"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_delay:
            wait_time = self.min_delay - time_since_last
            time.sleep(wait_time)
        
        self.last_request_time = time.time()
    
    def _make_request_groq(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """Requ√™te vers Groq API (gratuit, rapide) avec fallback automatique"""
        
        # Liste des mod√®les Groq gratuits √† tester
        groq_models = [
            "llama-3.3-70b-versatile",  # Plus r√©cent
            "llama-3.1-8b-instant",     # Plus rapide
            "mixtral-8x7b-32768",       # Bon contexte
            "gemma2-9b-it",             # Alternative
            "llama3-groq-70b-8192-tool-use-preview"  # Sp√©cialis√© tools
        ]
        
        for model in groq_models:
            try:
                self._wait_for_rate_limit()
                
                payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": max_tokens,
                    "temperature": self.config.TEMPERATURE
                }
                
                logger.info(f"üöÄ Requ√™te Groq (gratuit) - {model}")
                response = requests.post(
                    "https://api.groq.com/openai/v1/chat/completions",
                    headers=self.groq_headers,
                    json=payload,
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result["choices"][0]["message"]["content"]
                    logger.info(f"‚úÖ Requ√™te Groq r√©ussie avec {model}")
                    return content
                else:
                    logger.warning(f"‚ö†Ô∏è Groq {model} erreur {response.status_code}")
                    continue  # Essayer le mod√®le suivant
                    
            except Exception as e:
                logger.warning(f"‚ùå Erreur Groq {model}: {e}")
                continue  # Essayer le mod√®le suivant
        
        logger.error("‚ùå Tous les mod√®les Groq ont √©chou√©")
        return None
    
    def _make_request_ollama(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """Requ√™te vers Ollama local (gratuit)"""
        try:
            self._wait_for_rate_limit()
            
            payload = {
                "model": self.config.OLLAMA_MODEL,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": self.config.TEMPERATURE
                }
            }
            
            logger.info("üè† Requ√™te Ollama (local)")
            response = requests.post(
                f"{self.config.OLLAMA_BASE_URL}/api/generate",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get("response", "")
                logger.info("‚úÖ Requ√™te Ollama r√©ussie")
                return content
            else:
                logger.warning(f"‚ö†Ô∏è Ollama erreur {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erreur Ollama: {e}")
            return None
    
    def _make_request_mistral(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """Requ√™te vers Mistral (avec rate limiting am√©lior√©)"""
        try:
            # V√©rifier que Mistral est bien configur√©
            if not hasattr(self, 'client') or not hasattr(self, 'UserMessage'):
                logger.error("‚ùå Client Mistral non initialis√©")
                return None
            
            # D√©lai plus long pour Mistral
            time.sleep(3)
            
            messages = [self.UserMessage(content=prompt)]
            
            logger.info("ü§ñ Requ√™te Mistral")
            response = self.client.chat.complete(
                model=self.config.MISTRAL_MODEL,
                messages=messages,
                max_tokens=max_tokens,
                temperature=self.config.TEMPERATURE
            )
            
            logger.info("‚úÖ Requ√™te Mistral r√©ussie")
            return response.choices[0].message.content
            
        except Exception as e:
            error_str = str(e)
            if "429" in error_str or "Too Many Requests" in error_str:
                logger.warning("‚ö†Ô∏è Rate limit Mistral - basculement vers Groq")
                return self._make_request_groq(prompt, max_tokens)
            else:
                logger.error(f"‚ùå Erreur Mistral: {e}")
                return None
    
    def generate_completion(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """G√©n√®re une completion avec le provider configur√©"""
        
        if self.provider == "groq":
            return self._make_request_groq(prompt, max_tokens)
        elif self.provider == "ollama":
            return self._make_request_ollama(prompt, max_tokens)
        elif self.provider == "mistral":
            return self._make_request_mistral(prompt, max_tokens)
        else:
            logger.error(f"‚ùå Provider non support√©: {self.provider}")
            return None
    
    def generate_search_plan_legacy(self, user_query: str) -> Dict:
        """G√©n√®re un plan de recherche standard (OBSOL√àTE - utiliser generate_deep_search_plan)"""
        
        # Plan simple sans LLM si la requ√™te est courte (gain de temps)
        if len(user_query.split()) <= 3:
            logger.info("üìã Plan simple g√©n√©r√© (sans LLM)")
            base_query = user_query.strip()
            return {
                "requetes_recherche": [
                    base_query,
                    f"{base_query} avantages",
                    f"{base_query} inconv√©nients"
                ],
                "types_sources": ["articles sp√©cialis√©s", "√©tudes", "sites d'information"],
                "questions_secondaires": ["Quels sont les aspects importants ?"],
                "strategie": "Plan simple g√©n√©r√© automatiquement"
            }
        
        prompt = f"""Cr√©e 3 requ√™tes de recherche courtes pour: "{user_query}"

R√©ponds UNIQUEMENT avec les requ√™tes s√©par√©es par des virgules:
requ√™te1, requ√™te2, requ√™te3

Exemple pour "intelligence artificielle":
intelligence artificielle d√©finition, IA avantages inconv√©nients, intelligence artificielle applications"""

        content = self.generate_completion(prompt, max_tokens=150)
        
        if content:
            # Extraire les requ√™tes
            queries = [q.strip() for q in content.split(',') if q.strip()]
            
            if len(queries) >= 2:
                logger.info(f"üìã Plan LLM g√©n√©r√© avec {len(queries)} requ√™tes")
                return {
                    "requetes_recherche": queries[:4],  # Max 4 pour la vitesse
                    "types_sources": ["articles sp√©cialis√©s", "√©tudes", "sites d'information"],
                    "questions_secondaires": ["Quels sont les aspects importants ?"],
                    "strategie": f"Plan g√©n√©r√© par {self.provider.upper()}"
                }
        
        # Fallback simple
        logger.info("üìã Plan automatique g√©n√©r√©")
        base_query = user_query.strip()
        return {
            "requetes_recherche": [
                base_query,
                f"{base_query} 2024",
                f"{base_query} avantages"
            ],
            "types_sources": ["articles", "sites web"],
            "questions_secondaires": [],
            "strategie": "Plan automatique"
        }
    
    def generate_search_plan(self, user_query: str) -> Dict:
        """M√©thode de compatibilit√© - redirige vers generate_deep_search_plan"""
        logger.info("üîÑ Redirection vers plan intelligent")
        return self.generate_deep_search_plan(user_query)
    
    def generate_deep_search_plan(self, user_query: str) -> Dict:
        """G√©n√®re un plan de recherche intelligent avec analyse JSON"""
        
        prompt = f"""G√©n√®re un plan de recherche web intelligent pour la question suivante : "{user_query}"

Analyse d'abord la question pour comprendre ce que l'utilisateur demande vraiment, puis cr√©e un plan structur√©.

Retourne le r√©sultat au format JSON strict avec les champs suivants :
- "analyse": une phrase d√©crivant ce que l'utilisateur cherche vraiment
- "plan": une liste de 3-4 √©tapes logiques de recherche  
- "requetes_recherche": une liste de 5-6 requ√™tes Google pr√©cises et pertinentes (en fran√ßais)
- "questions_secondaires": une liste de 2-3 questions secondaires importantes
- "strategie": description de l'approche utilis√©e

Exemples de questions et leurs analyses :

Pour "qui est le plus riche entre elon musk et fran√ßois hollande" :
{{"analyse": "Comparaison de patrimoine entre un milliardaire am√©ricain et un ex-pr√©sident fran√ßais", "plan": ["Rechercher fortune actuelle Elon Musk", "Rechercher patrimoine Fran√ßois Hollande", "Comparer les montants", "Analyser les sources de richesse"], "requetes_recherche": ["Elon Musk fortune 2024 milliards", "Fran√ßois Hollande patrimoine net worth", "richest people 2024 Musk classement", "patrimoine pr√©sident France Hollande", "Tesla SpaceX valeur Musk fortune", "salaire pr√©sident France vs milliardaires"], "questions_secondaires": ["Quelles sont leurs sources de revenus principales ?", "Comment se situe Hollande par rapport aux autres politiques ?", "√âvolution fortune Musk derni√®res ann√©es ?"], "strategie": "Recherche comparative de donn√©es financi√®res publiques"}}

Pour "comment dresser un chien agressif" :
{{"analyse": "Techniques d'√©ducation canine pour corriger comportements agressifs", "plan": ["Identifier causes agressivit√©", "Techniques de dressage sp√©cialis√©es", "Conseils v√©t√©rinaires/experts", "T√©moignages propri√©taires"], "requetes_recherche": ["chien agressif dressage techniques", "√©ducateur canin agression solutions", "v√©t√©rinaire comportementaliste chien", "socialisation chien adulte agressif", "m√©thodes √©ducation positive chien", "chien mord que faire conseils"], "questions_secondaires": ["Quand consulter un professionnel ?", "Quels sont les signes pr√©curseurs ?", "Peut-on √©viter l'agressivit√© ?"], "strategie": "Approche multi-expertise (v√©t√©rinaire, √©ducation, comportement)"}}

R√©ponds UNIQUEMENT avec le JSON valide, sans texte additionnel."""

        # Forcer le format JSON si le provider le supporte
        if self.provider == "mistral":
            content = self._make_request_mistral_json(prompt, max_tokens=800)
        else:
            content = self.generate_completion(prompt, max_tokens=800)
        
        if content:
            try:
                # Nettoyer le contenu pour extraire le JSON
                content = content.strip()
                if content.startswith('```json'):
                    content = content[7:-3]
                elif content.startswith('```'):
                    content = content[3:-3]
                
                import json
                plan_data = json.loads(content)
                
                # Validation des champs requis
                required_fields = ["analyse", "plan", "requetes_recherche", "questions_secondaires", "strategie"]
                if all(field in plan_data for field in required_fields):
                    # Limiter le nombre de requ√™tes
                    plan_data["requetes_recherche"] = plan_data["requetes_recherche"][:6]
                    
                    # Affichage d√©taill√© du plan dans la console avec couleurs
                    colored_log("info", "üìã ========= PLAN DE RECHERCHE INTELLIGENT =========", Fore.CYAN + Style.BRIGHT)
                    colored_log("info", f"üéØ ANALYSE: {plan_data['analyse']}", Fore.YELLOW + Style.BRIGHT)
                    colored_log("info", "üìä √âTAPES DU PLAN:", Fore.GREEN + Style.BRIGHT)
                    for i, etape in enumerate(plan_data['plan'], 1):
                        colored_log("info", f"   {i}. {etape}", Fore.GREEN)
                    colored_log("info", "üîç REQU√äTES DE RECHERCHE:", Fore.BLUE + Style.BRIGHT)
                    for i, query in enumerate(plan_data['requetes_recherche'], 1):
                        colored_log("info", f"   {i}. '{query}'", Fore.BLUE)
                    colored_log("info", "‚ùì QUESTIONS SECONDAIRES:", Fore.MAGENTA + Style.BRIGHT)
                    for question in plan_data['questions_secondaires']:
                        colored_log("info", f"   ‚Ä¢ {question}", Fore.MAGENTA)
                    colored_log("info", f"üé≤ STRAT√âGIE: {plan_data['strategie']}", Fore.WHITE + Style.BRIGHT)
                    colored_log("info", "=" * 55, Fore.CYAN + Style.BRIGHT)
                    
                    return {
                        "requetes_recherche": plan_data["requetes_recherche"],
                        "types_sources": ["donn√©es officielles", "sites sp√©cialis√©s", "√©tudes", "articles de r√©f√©rence"],
                        "questions_secondaires": plan_data["questions_secondaires"],
                        "strategie": f"Plan intelligent: {plan_data['strategie']}",
                        "analyse": plan_data["analyse"],
                        "plan_etapes": plan_data["plan"]
                    }
                    
            except (json.JSONDecodeError, KeyError) as e:
                logger.warning(f"‚ö†Ô∏è Erreur parsing JSON plan: {e}")
        
        # Fallback intelligent bas√© sur l'analyse de la question
        logger.info("üìã Plan de fallback intelligent")
        return self._generate_smart_fallback_plan(user_query)
    
    def _make_request_mistral_json(self, prompt: str, max_tokens: int = 800) -> Optional[str]:
        """Requ√™te Mistral avec format JSON forc√©"""
        if not self.config.MISTRAL_API_KEY:
            return None
            
        try:
            self._wait_for_rate_limit()
            
            payload = {
                "model": self.config.MISTRAL_MODEL,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": self.config.TEMPERATURE,
                "response_format": {"type": "json_object"}
            }
            
            logger.info("ü§ñ Requ√™te Mistral JSON")
            response = requests.post(
                "https://api.mistral.ai/v1/chat/completions",
                headers=self.mistral_headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                logger.info("‚úÖ Requ√™te Mistral JSON r√©ussie")
                return content
            elif response.status_code == 429:
                logger.warning("‚ö†Ô∏è Rate limit Mistral, attente...")
                time.sleep(5)
                return None
            else:
                logger.warning(f"‚ö†Ô∏è Erreur Mistral JSON: {response.status_code}")
                return None
                
        except Exception as e:
            logger.warning(f"‚ùå Erreur requ√™te Mistral JSON: {e}")
            return None
    
    def _generate_smart_fallback_plan(self, user_query: str) -> Dict:
        """G√©n√®re un plan de fallback intelligent bas√© sur l'analyse de mots-cl√©s"""
        query_lower = user_query.lower()
        
        # D√©tection du type de question
        if any(word in query_lower for word in ["qui est", "plus riche", "fortune", "patrimoine", "richesse"]):
            # Question de comparaison financi√®re
            base_terms = ["fortune", "patrimoine", "richesse", "net worth"]
            if "musk" in query_lower and "hollande" in query_lower:
                return {
                    "requetes_recherche": [
                        "Elon Musk fortune 2024 milliards",
                        "Fran√ßois Hollande patrimoine d√©claration",
                        "richest people world 2024 Forbes",
                        "pr√©sident France salaire patrimoine",
                        "Tesla SpaceX valeur Musk",
                        "comparaison fortune politiques milliardaires"
                    ],
                    "types_sources": ["Forbes", "sites financiers", "d√©clarations officielles"],
                    "questions_secondaires": ["Sources de revenus de chacun ?", "√âvolution dans le temps ?"],
                    "strategie": "Fallback intelligent: comparaison financi√®re",
                    "analyse": "Comparaison de patrimoine entre personnalit√©s publiques",
                    "plan_etapes": ["Recherche fortune Musk", "Recherche patrimoine Hollande", "Comparaison", "Contexte"]
                }
        
        elif any(word in query_lower for word in ["comment", "dresser", "√©duquer", "apprendre"]):
            # Question pratique/tutoriel
            return {
                "requetes_recherche": [
                    f"{user_query} guide",
                    f"{user_query} conseils experts",
                    f"{user_query} m√©thode √©tapes",
                    f"{user_query} erreurs √©viter",
                    f"{user_query} t√©moignages",
                    f"{user_query} 2024 techniques"
                ],
                "types_sources": ["guides pratiques", "sites sp√©cialis√©s", "forums"],
                "questions_secondaires": ["Quelles erreurs √©viter ?", "Combien de temps √ßa prend ?"],
                "strategie": "Fallback intelligent: guide pratique",
                "analyse": "Recherche de conseils et m√©thodes pratiques",
                "plan_etapes": ["M√©thodes de base", "Conseils experts", "T√©moignages", "Erreurs courantes"]
            }
        
        # Fallback g√©n√©ral
        base_query = user_query.strip()
        return {
            "requetes_recherche": [
                base_query,
                f"{base_query} guide complet",
                f"{base_query} conseils experts",
                f"{base_query} 2024 actualit√©s",
                f"{base_query} avantages inconv√©nients",
                f"{base_query} t√©moignages avis"
            ],
            "types_sources": ["articles de r√©f√©rence", "sites sp√©cialis√©s"],
            "questions_secondaires": ["Quels sont les points cl√©s ?", "Quelles sont les tendances ?"],
            "strategie": "Fallback g√©n√©ral intelligent",
            "analyse": f"Recherche d'informations compl√®tes sur: {base_query}",
            "plan_etapes": ["Informations g√©n√©rales", "Avis experts", "Actualit√©s", "Retours utilisateurs"]
        }
    
    def generate_contextual_search_plan(self, context_prompt: str, context_result: Dict) -> Dict:
        """G√©n√®re un plan de recherche enrichi avec le contexte pr√©c√©dent"""
        
        # Extraire des informations du contexte
        original_query = context_result.get('query', '')
        original_plan = context_result.get('plan', {})
        
        prompt = f"""G√©n√®re un plan de recherche contextuel intelligent bas√© sur ce contexte :

{context_prompt}

La nouvelle recherche doit √™tre COMPL√âMENTAIRE et ENRICHIR les r√©sultats pr√©c√©dents, pas les r√©p√©ter.

Retourne le r√©sultat au format JSON strict avec les champs suivants :
- "analyse": analyse de la question de suivi dans son contexte
- "plan": liste de 3 √©tapes logiques pour cette recherche contextuelle
- "requetes_recherche": liste de 4-5 requ√™tes Google SP√âCIFIQUES √† la question de suivi (√©viter de r√©p√©ter les requ√™tes d√©j√† faites)
- "questions_secondaires": 2 questions secondaires pour approfondir
- "strategie": description de l'approche contextuelle utilis√©e

Exemple de r√©ponse pour une question de suivi "Quels sont les risques ?" apr√®s une recherche sur "intelligence artificielle avantages" :
{{"analyse": "L'utilisateur veut maintenant conna√Ætre les risques de l'IA apr√®s avoir vu les avantages", "plan": ["Identifier les risques principaux de l'IA", "Rechercher des cas concrets de probl√®mes", "Analyser les mesures de pr√©vention"], "requetes_recherche": ["intelligence artificielle risques dangers", "IA biais algorithmes probl√®mes", "intelligence artificielle √©thique limites", "AI safety s√©curit√© risques", "intelligence artificielle emploi menaces"], "questions_secondaires": ["Comment minimiser ces risques ?", "Quels secteurs sont le plus √† risque ?"], "strategie": "Recherche contextuelle cibl√©e sur les aspects n√©gatifs pour compl√©ter la vision pr√©c√©dente"}}

R√©ponds UNIQUEMENT avec le JSON valide, sans texte additionnel."""

        # Forcer le format JSON si le provider le supporte
        if self.provider == "mistral":
            content = self._make_request_mistral_json(prompt, max_tokens=800)
        else:
            content = self.generate_completion(prompt, max_tokens=800)
        
        if content:
            try:
                # Nettoyer le contenu pour extraire le JSON
                content = content.strip()
                if content.startswith('```json'):
                    content = content[7:-3]
                elif content.startswith('```'):
                    content = content[3:-3]
                
                import json
                plan_data = json.loads(content)
                
                # Validation des champs requis
                required_fields = ["analyse", "plan", "requetes_recherche", "questions_secondaires", "strategie"]
                if all(field in plan_data for field in required_fields):
                    # Limiter le nombre de requ√™tes
                    plan_data["requetes_recherche"] = plan_data["requetes_recherche"][:5]
                    
                    # Affichage du plan contextuel dans la console
                    colored_log("info", "üîó ========= PLAN DE RECHERCHE CONTEXTUEL =========", Fore.CYAN + Style.BRIGHT)
                    colored_log("info", f"üéØ CONTEXTE: Bas√© sur \"{original_query}\"", Fore.YELLOW + Style.BRIGHT)
                    colored_log("info", f"üß† ANALYSE: {plan_data['analyse']}", Fore.YELLOW + Style.BRIGHT)
                    colored_log("info", "üìä √âTAPES CONTEXTUELLES:", Fore.GREEN + Style.BRIGHT)
                    for i, etape in enumerate(plan_data['plan'], 1):
                        colored_log("info", f"   {i}. {etape}", Fore.GREEN)
                    colored_log("info", "üîç NOUVELLES REQU√äTES:", Fore.BLUE + Style.BRIGHT)
                    for i, query in enumerate(plan_data['requetes_recherche'], 1):
                        colored_log("info", f"   {i}. '{query}'", Fore.BLUE)
                    colored_log("info", f"üé≤ STRAT√âGIE CONTEXTUELLE: {plan_data['strategie']}", Fore.WHITE + Style.BRIGHT)
                    colored_log("info", "=" * 55, Fore.CYAN + Style.BRIGHT)
                    
                    return {
                        "requetes_recherche": plan_data["requetes_recherche"],
                        "types_sources": ["sources compl√©mentaires", "nouveaux points de vue", "analyses sp√©cialis√©es"],
                        "questions_secondaires": plan_data["questions_secondaires"],
                        "strategie": f"Plan contextuel: {plan_data['strategie']}",
                        "analyse": plan_data["analyse"],
                        "plan_etapes": plan_data["plan"],
                        "is_contextual": True
                    }
                    
            except (json.JSONDecodeError, KeyError) as e:
                logger.warning(f"‚ö†Ô∏è Erreur parsing JSON plan contextuel: {e}")
        
        # Fallback contextuel intelligent
        logger.info("üìã Plan contextuel de fallback")
        return self._generate_contextual_fallback_plan(context_prompt, context_result)

    def _generate_contextual_fallback_plan(self, context_prompt: str, context_result: Dict) -> Dict:
        """G√©n√®re un plan de fallback contextuel bas√© sur l'analyse de la question de suivi"""
        
        # Extraire la question de suivi du prompt
        followup_query = ""
        lines = context_prompt.split('\n')
        for line in lines:
            if line.startswith('Question de suivi:'):
                followup_query = line.replace('Question de suivi:', '').strip().strip('"')
                break
        
        query_lower = followup_query.lower()
        original_query = context_result.get('query', '')
        
        # Prendre seulement les 2-3 premiers mots de la requ√™te originale pour √©viter les requ√™tes trop longues
        base_terms = original_query.split()[:3]  
        base_query = " ".join(base_terms)
        
        # Extraire les mots-cl√©s principaux de la question de suivi
        followup_keywords = []
        words = followup_query.split()[:3]  # Limiter √† 3 mots
        for word in words:
            if len(word) > 2 and word.lower() not in ['les', 'des', 'une', 'est', 'sont', 'avec', 'dans', 'pour']:
                followup_keywords.append(word)
        
        # G√©n√©rer des requ√™tes simples et efficaces
        if any(word in query_lower for word in ["risque", "danger", "probl√®me", "inconv√©nient"]):
            # Question sur les risques - requ√™tes courtes et cibl√©es
            return {
                "requetes_recherche": [
                    f"{base_query} risques",
                    f"{base_query} dangers",
                    f"{base_query} probl√®mes",
                    f"{base_query} pr√©cautions"
                ],
                "types_sources": ["√©tudes sur les risques", "rapports de s√©curit√©"],
                "questions_secondaires": ["Comment minimiser les risques ?", "Dans quels cas √©viter ?"],
                "strategie": "Fallback contextuel: focus sur les aspects n√©gatifs",
                "analyse": f"Recherche des risques li√©s √† {base_query}",
                "plan_etapes": ["Identifier les risques", "Analyser les causes", "Trouver des solutions"],
                "is_contextual": True
            }
        
        elif any(word in query_lower for word in ["exemple", "cas", "concret", "pratique"]):
            # Question sur des exemples pratiques
            return {
                "requetes_recherche": [
                    f"{base_query} exemples",
                    f"{base_query} cas pratiques",
                    f"{base_query} t√©moignages",
                    f"{base_query} exp√©riences"
                ],
                "types_sources": ["t√©moignages", "√©tudes de cas"],
                "questions_secondaires": ["Quels sont les r√©sultats ?", "Combien de temps ?"],
                "strategie": "Fallback contextuel: recherche d'exemples concrets",
                "analyse": f"Recherche d'exemples pratiques pour {base_query}",
                "plan_etapes": ["Trouver des cas concrets", "Analyser les r√©sultats", "Identifier les facteurs"],
                "is_contextual": True
            }
        
        elif any(word in query_lower for word in ["alternative", "autre", "diff√©rent", "comparaison"]):
            # Question sur les alternatives
            return {
                "requetes_recherche": [
                    f"{base_query} alternatives",
                    f"{base_query} options",
                    f"{base_query} comparaison",
                    f"alternative {base_query}"
                ],
                "types_sources": ["guides comparatifs", "analyses d'alternatives"],
                "questions_secondaires": ["Quels crit√®res choisir ?", "Quelle est la meilleure option ?"],
                "strategie": "Fallback contextuel: recherche d'alternatives",
                "analyse": f"Recherche d'alternatives √† {base_query}",
                "plan_etapes": ["Identifier alternatives", "Comparer options", "√âvaluer crit√®res"],
                "is_contextual": True
            }
        
        # Fallback g√©n√©ral contextuel - requ√™tes tr√®s simples
        main_keyword = followup_keywords[0] if followup_keywords else "informations"
        
        return {
            "requetes_recherche": [
                f"{base_query} {main_keyword}",
                f"{main_keyword} {base_query}",
                f"{base_query} guide",
                f"{base_query} conseils"
            ],
            "types_sources": ["informations compl√©mentaires", "guides pratiques"],
            "questions_secondaires": ["Quels autres aspects ?", "Nuances importantes ?"],
            "strategie": "Fallback contextuel g√©n√©ral",
            "analyse": f"Approfondissement de {base_query} avec focus sur {main_keyword}",
            "plan_etapes": ["Recherche compl√©mentaire", "Analyse", "Synth√®se"],
            "is_contextual": True
        }

    def synthesize_contextual_results(self, followup_query: str, search_results: List[Dict], scraped_articles: List[Dict], context_result: Dict) -> str:
        """Synth√©tise les r√©sultats d'une recherche contextuelle en int√©grant le contexte pr√©c√©dent"""
        
        original_query = context_result.get('query', '')
        original_synthesis = context_result.get('synthesis', '')
        
        # Pr√©parer le contexte pour le prompt
        context_summary = original_synthesis[:500] + "..." if len(original_synthesis) > 500 else original_synthesis
        
        # Pr√©parer les r√©sultats de recherche
        search_summary = self._prepare_search_summary(search_results)
        articles_content = self._prepare_articles_content(scraped_articles)
        
        prompt = f"""Tu es un expert en recherche et synth√®se d'informations. 

CONTEXTE DE LA RECHERCHE PR√âC√âDENTE:
Question originale: "{original_query}"
Synth√®se pr√©c√©dente: {context_summary}

NOUVELLE QUESTION DE SUIVI: "{followup_query}"

NOUVEAUX R√âSULTATS DE RECHERCHE:
{search_summary}

NOUVEAUX ARTICLES ANALYS√âS:
{articles_content}

INSTRUCTIONS:
1. R√©ponds SP√âCIFIQUEMENT √† la question de suivi "{followup_query}"
2. INT√àGRE intelligemment les informations de la recherche pr√©c√©dente quand c'est pertinent
3. METS EN √âVIDENCE les connexions entre les r√©sultats pr√©c√©dents et les nouveaux
4. Structure ta r√©ponse de mani√®re claire et compl√®te
5. Indique quand tu enrichis ou nuances les informations pr√©c√©dentes

Format de r√©ponse souhait√©:
- Introduction rappelant le lien avec la recherche pr√©c√©dente
- R√©ponse d√©taill√©e √† la question de suivi
- Connexions et nuances par rapport aux r√©sultats pr√©c√©dents
- Conclusion synth√©tique

√âcris en fran√ßais et sois pr√©cis et informatif."""

        try:
            synthesis = self.generate_completion(prompt, max_tokens=1000)
            
            if synthesis:
                # Ajouter un header contextuel
                contextual_header = f"**üîó Recherche contextuelle bas√©e sur:** \"{original_query}\"\n\n"
                return contextual_header + synthesis
            else:
                return self._generate_fallback_contextual_synthesis(followup_query, original_query, search_results, scraped_articles)
                
        except Exception as e:
            logger.error(f"‚ùå Erreur synth√®se contextuelle: {e}")
            return self._generate_fallback_contextual_synthesis(followup_query, original_query, search_results, scraped_articles)

    def _generate_fallback_contextual_synthesis(self, followup_query: str, original_query: str, search_results: List[Dict], scraped_articles: List[Dict]) -> str:
        """G√©n√®re une synth√®se contextuelle de fallback"""
        
        synthesis = f"**üîó Recherche contextuelle bas√©e sur:** \"{original_query}\"\n\n"
        synthesis += f"**R√©ponse √† votre question de suivi:** {followup_query}\n\n"
        
        if search_results:
            synthesis += f"**üìä Nouveaux r√©sultats trouv√©s:** {len(search_results)} sources analys√©es\n\n"
            
            # R√©sum√© des points cl√©s des r√©sultats
            synthesis += "**üîç Points cl√©s identifi√©s:**\n"
            for i, result in enumerate(search_results[:5], 1):
                snippet = result.get('snippet', '')[:150] + "..." if len(result.get('snippet', '')) > 150 else result.get('snippet', '')
                synthesis += f"{i}. {snippet}\n"
            synthesis += "\n"
        
        if scraped_articles:
            synthesis += f"**üì∞ Articles analys√©s en d√©tail:** {len(scraped_articles)} articles\n\n"
            
            # Extraits des articles les plus pertinents
            synthesis += "**üí° Informations compl√©mentaires:**\n"
            for i, article in enumerate(scraped_articles[:3], 1):
                content = article.get('content', '')[:200] + "..." if len(article.get('content', '')) > 200 else article.get('content', '')
                synthesis += f"‚Ä¢ **{article.get('title', 'Article')}:** {content}\n"
            synthesis += "\n"
        
        synthesis += "**üéØ Cette recherche contextuelle vient enrichir vos connaissances pr√©c√©dentes "
        synthesis += f"sur \"{original_query}\" en apportant des √©l√©ments sp√©cifiques √† votre question de suivi.**"
        
        return synthesis

    def _prepare_search_summary(self, search_results: List[Dict]) -> str:
        """Pr√©pare un r√©sum√© des r√©sultats de recherche pour le prompt"""
        if not search_results:
            return "Aucun r√©sultat de recherche trouv√©."
        
        summary = f"R√©sultats trouv√©s ({len(search_results)} sources):\n"
        for i, result in enumerate(search_results[:10], 1):  # Limiter √† 10 pour √©viter un prompt trop long
            title = result.get('title', 'Titre non disponible')
            snippet = result.get('snippet', 'Extrait non disponible')[:200]
            summary += f"{i}. {title}\n   {snippet}...\n"
        
        return summary

    def _prepare_articles_content(self, scraped_articles: List[Dict]) -> str:
        """Pr√©pare le contenu des articles scrap√©s pour le prompt"""
        if not scraped_articles:
            return "Aucun article analys√© en d√©tail."
        
        content = f"Articles analys√©s ({len(scraped_articles)} articles):\n"
        for i, article in enumerate(scraped_articles[:5], 1):  # Limiter √† 5 articles
            title = article.get('title', 'Titre non disponible')
            article_content = article.get('content', 'Contenu non disponible')[:300]  # Limiter la taille
            content += f"{i}. {title}\n   {article_content}...\n"
        
        return content

    def synthesize_results(self, query: str, search_results: List[Dict], scraped_articles: List[Dict]) -> str:
        """Synth√©tise les r√©sultats de recherche"""
        
        # Pr√©parer le contexte
        context = f"Question: {query}\n\n"
        context += "Sources trouv√©es:\n"
        
        for i, result in enumerate(search_results[:5], 1):
            context += f"{i}. {result.get('title', 'N/A')}\n"
            context += f"   {result.get('snippet', 'N/A')[:200]}...\n\n"
        
        if scraped_articles:
            context += "\nArticles analys√©s:\n"
            for i, article in enumerate(scraped_articles[:3], 1):
                context += f"{i}. {article.get('title', 'N/A')}\n"
                context += f"   {article.get('content', 'N/A')[:300]}...\n\n"
        
        prompt = f"""{context}

Bas√© sur ces informations, r√©dige une synth√®se claire et structur√©e r√©pondant √†: "{query}"

Structure:
- Introduction courte
- Points cl√©s (3-4 points)
- Conclusion

Reste factuel et cite les sources quand pertinent."""

        content = self.generate_completion(prompt, max_tokens=800)
        
        if content:
            return content
        else:
            # Fallback simple
            return f"""Synth√®se pour: {query}

Bas√© sur {len(search_results)} sources trouv√©es et {len(scraped_articles)} articles analys√©s, voici les informations principales:

{search_results[0].get('snippet', 'Informations non disponibles') if search_results else 'Aucune source trouv√©e'}

Note: Cette synth√®se a √©t√© g√©n√©r√©e automatiquement en raison d'un probl√®me temporaire avec l'IA.""" 