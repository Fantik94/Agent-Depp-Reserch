import requests
import json
import time
import logging
from typing import List, Dict, Optional
from config import Config

# Couleurs pour les logs
try:
    from colorama import init, Fore, Back, Style
    init(autoreset=True)
    COLORS_AVAILABLE = True
except ImportError:
    COLORS_AVAILABLE = False
    Fore = Back = Style = type('', (), {'__getattr__': lambda self, name: ''})()

logger = logging.getLogger(__name__)

def colored_log(level, message, color=None):
    """Log avec couleur si disponible"""
    if COLORS_AVAILABLE and color:
        print(f"{color}{message}{Style.RESET_ALL}")
    else:
        getattr(logger, level)(message)

class UniversalLLMClient:
    """Client LLM universel supportant Mistral, Groq et Ollama"""
    
    def __init__(self, provider: str = None):
        self.config = Config()
        self.provider = provider or self.config.LLM_PROVIDER
        self.last_request_time = 0
        self.min_delay = 1  # D√©lai r√©duit
        
        # Initialiser le client selon le provider
        if self.provider == "mistral":
            try:
                from mistralai import Mistral
                from mistralai.models import UserMessage
                self.client = Mistral(api_key=self.config.MISTRAL_API_KEY)
                self.UserMessage = UserMessage
                # Headers pour les requ√™tes directes
                self.mistral_headers = {
                    "Authorization": f"Bearer {self.config.MISTRAL_API_KEY}",
                    "Content-Type": "application/json"
                }
                logger.info(f"ü§ñ Client Mistral initialis√© avec {self.config.MISTRAL_MODEL}")
            except Exception as e:
                logger.error(f"‚ùå Erreur initialisation Mistral: {e}")
                logger.info("üîÑ Basculement vers Groq")
                self.provider = "groq"  # Fallback vers Groq
                self._init_groq()
        elif self.provider == "groq":
            self._init_groq()
        elif self.provider == "ollama":
            # Ollama local
            pass
        else:
            logger.warning(f"‚ö†Ô∏è Provider inconnu: {self.provider}, utilisation de Groq")
            self.provider = "groq"
            self._init_groq()
            
        logger.info(f"ü§ñ Client LLM initialis√©: {self.provider}")
    
    def _init_groq(self):
        """Initialise Groq avec les headers n√©cessaires"""
        if not self.config.GROQ_API_KEY:
            logger.error("‚ùå Cl√© API Groq manquante")
            return
        
        self.groq_headers = {
            "Authorization": f"Bearer {self.config.GROQ_API_KEY}",
            "Content-Type": "application/json"
        }
        logger.info("ü§ñ Headers Groq initialis√©s")
    
    def _wait_for_rate_limit(self):
        """Attend avant la prochaine requ√™te (d√©lai r√©duit)"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_delay:
            wait_time = self.min_delay - time_since_last
            time.sleep(wait_time)
        
        self.last_request_time = time.time()
    
    def _make_request_groq(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """Requ√™te vers Groq API (gratuit, rapide) avec fallback automatique"""
        
        # Liste des mod√®les Groq gratuits √† tester
        groq_models = [
            "llama-3.3-70b-versatile",  # Plus r√©cent
            "llama-3.1-8b-instant",     # Plus rapide
            "mixtral-8x7b-32768",       # Bon contexte
            "gemma2-9b-it",             # Alternative
            "llama3-groq-70b-8192-tool-use-preview"  # Sp√©cialis√© tools
        ]
        
        for model in groq_models:
            try:
                self._wait_for_rate_limit()
                
                payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": max_tokens,
                    "temperature": self.config.TEMPERATURE
                }
                
                logger.info(f"üöÄ Requ√™te Groq (gratuit) - {model}")
                response = requests.post(
                    "https://api.groq.com/openai/v1/chat/completions",
                    headers=self.groq_headers,
                    json=payload,
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result["choices"][0]["message"]["content"]
                    logger.info(f"‚úÖ Requ√™te Groq r√©ussie avec {model}")
                    return content
                else:
                    logger.warning(f"‚ö†Ô∏è Groq {model} erreur {response.status_code}")
                    continue  # Essayer le mod√®le suivant
                    
            except Exception as e:
                logger.warning(f"‚ùå Erreur Groq {model}: {e}")
                continue  # Essayer le mod√®le suivant
        
        logger.error("‚ùå Tous les mod√®les Groq ont √©chou√©")
        return None
    
    def _make_request_ollama(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """Requ√™te vers Ollama local (gratuit)"""
        try:
            self._wait_for_rate_limit()
            
            payload = {
                "model": self.config.OLLAMA_MODEL,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": self.config.TEMPERATURE
                }
            }
            
            logger.info("üè† Requ√™te Ollama (local)")
            response = requests.post(
                f"{self.config.OLLAMA_BASE_URL}/api/generate",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get("response", "")
                logger.info("‚úÖ Requ√™te Ollama r√©ussie")
                return content
            else:
                logger.warning(f"‚ö†Ô∏è Ollama erreur {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erreur Ollama: {e}")
            return None
    
    def _make_request_mistral(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """Requ√™te vers Mistral (avec rate limiting am√©lior√©)"""
        try:
            # V√©rifier que Mistral est bien configur√©
            if not hasattr(self, 'client') or not hasattr(self, 'UserMessage'):
                logger.error("‚ùå Client Mistral non initialis√©")
                return None
            
            # D√©lai plus long pour Mistral
            time.sleep(3)
            
            messages = [self.UserMessage(content=prompt)]
            
            logger.info("ü§ñ Requ√™te Mistral")
            response = self.client.chat.complete(
                model=self.config.MISTRAL_MODEL,
                messages=messages,
                max_tokens=max_tokens,
                temperature=self.config.TEMPERATURE
            )
            
            logger.info("‚úÖ Requ√™te Mistral r√©ussie")
            return response.choices[0].message.content
            
        except Exception as e:
            error_str = str(e)
            if "429" in error_str or "Too Many Requests" in error_str:
                logger.warning("‚ö†Ô∏è Rate limit Mistral - basculement vers Groq")
                return self._make_request_groq(prompt, max_tokens)
            else:
                logger.error(f"‚ùå Erreur Mistral: {e}")
                return None
    
    def generate_completion(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """G√©n√®re une completion avec le provider configur√©"""
        
        if self.provider == "groq":
            return self._make_request_groq(prompt, max_tokens)
        elif self.provider == "ollama":
            return self._make_request_ollama(prompt, max_tokens)
        elif self.provider == "mistral":
            return self._make_request_mistral(prompt, max_tokens)
        else:
            logger.error(f"‚ùå Provider non support√©: {self.provider}")
            return None
    
    def generate_search_plan_legacy(self, user_query: str) -> Dict:
        """G√©n√®re un plan de recherche standard (OBSOL√àTE - utiliser generate_deep_search_plan)"""
        
        # Plan simple sans LLM si la requ√™te est courte (gain de temps)
        if len(user_query.split()) <= 3:
            logger.info("üìã Plan simple g√©n√©r√© (sans LLM)")
            base_query = user_query.strip()
            return {
                "requetes_recherche": [
                    base_query,
                    f"{base_query} avantages",
                    f"{base_query} inconv√©nients"
                ],
                "types_sources": ["articles sp√©cialis√©s", "√©tudes", "sites d'information"],
                "questions_secondaires": ["Quels sont les aspects importants ?"],
                "strategie": "Plan simple g√©n√©r√© automatiquement"
            }
        
        prompt = f"""Cr√©e 3 requ√™tes de recherche courtes pour: "{user_query}"

R√©ponds UNIQUEMENT avec les requ√™tes s√©par√©es par des virgules:
requ√™te1, requ√™te2, requ√™te3

Exemple pour "intelligence artificielle":
intelligence artificielle d√©finition, IA avantages inconv√©nients, intelligence artificielle applications"""

        content = self.generate_completion(prompt, max_tokens=150)
        
        if content:
            # Extraire les requ√™tes
            queries = [q.strip() for q in content.split(',') if q.strip()]
            
            if len(queries) >= 2:
                logger.info(f"üìã Plan LLM g√©n√©r√© avec {len(queries)} requ√™tes")
                return {
                    "requetes_recherche": queries[:4],  # Max 4 pour la vitesse
                    "types_sources": ["articles sp√©cialis√©s", "√©tudes", "sites d'information"],
                    "questions_secondaires": ["Quels sont les aspects importants ?"],
                    "strategie": f"Plan g√©n√©r√© par {self.provider.upper()}"
                }
        
        # Fallback simple
        logger.info("üìã Plan automatique g√©n√©r√©")
        base_query = user_query.strip()
        return {
            "requetes_recherche": [
                base_query,
                f"{base_query} 2024",
                f"{base_query} avantages"
            ],
            "types_sources": ["articles", "sites web"],
            "questions_secondaires": [],
            "strategie": "Plan automatique"
        }
    
    def generate_search_plan(self, user_query: str) -> Dict:
        """M√©thode de compatibilit√© - redirige vers generate_deep_search_plan"""
        logger.info("üîÑ Redirection vers plan intelligent")
        return self.generate_deep_search_plan(user_query)
    
    def generate_deep_search_plan(self, user_query: str) -> Dict:
        """G√©n√®re un plan de recherche intelligent avec analyse JSON"""
        
        prompt = f"""G√©n√®re un plan de recherche web intelligent pour la question suivante : "{user_query}"

Analyse d'abord la question pour comprendre ce que l'utilisateur demande vraiment, puis cr√©e un plan structur√©.

Retourne le r√©sultat au format JSON strict avec les champs suivants :
- "analyse": une phrase d√©crivant ce que l'utilisateur cherche vraiment
- "plan": une liste de 3-4 √©tapes logiques de recherche  
- "requetes_recherche": une liste de 5-6 requ√™tes Google pr√©cises et pertinentes (en fran√ßais)
- "questions_secondaires": une liste de 2-3 questions secondaires importantes
- "strategie": description de l'approche utilis√©e

Exemples de questions et leurs analyses :

Pour "qui est le plus riche entre elon musk et fran√ßois hollande" :
{{"analyse": "Comparaison de patrimoine entre un milliardaire am√©ricain et un ex-pr√©sident fran√ßais", "plan": ["Rechercher fortune actuelle Elon Musk", "Rechercher patrimoine Fran√ßois Hollande", "Comparer les montants", "Analyser les sources de richesse"], "requetes_recherche": ["Elon Musk fortune 2024 milliards", "Fran√ßois Hollande patrimoine net worth", "richest people 2024 Musk classement", "patrimoine pr√©sident France Hollande", "Tesla SpaceX valeur Musk fortune", "salaire pr√©sident France vs milliardaires"], "questions_secondaires": ["Quelles sont leurs sources de revenus principales ?", "Comment se situe Hollande par rapport aux autres politiques ?", "√âvolution fortune Musk derni√®res ann√©es ?"], "strategie": "Recherche comparative de donn√©es financi√®res publiques"}}

Pour "comment dresser un chien agressif" :
{{"analyse": "Techniques d'√©ducation canine pour corriger comportements agressifs", "plan": ["Identifier causes agressivit√©", "Techniques de dressage sp√©cialis√©es", "Conseils v√©t√©rinaires/experts", "T√©moignages propri√©taires"], "requetes_recherche": ["chien agressif dressage techniques", "√©ducateur canin agression solutions", "v√©t√©rinaire comportementaliste chien", "socialisation chien adulte agressif", "m√©thodes √©ducation positive chien", "chien mord que faire conseils"], "questions_secondaires": ["Quand consulter un professionnel ?", "Quels sont les signes pr√©curseurs ?", "Peut-on √©viter l'agressivit√© ?"], "strategie": "Approche multi-expertise (v√©t√©rinaire, √©ducation, comportement)"}}

R√©ponds UNIQUEMENT avec le JSON valide, sans texte additionnel."""

        # Forcer le format JSON si le provider le supporte
        if self.provider == "mistral":
            content = self._make_request_mistral_json(prompt, max_tokens=800)
        else:
            content = self.generate_completion(prompt, max_tokens=800)
        
        if content:
            try:
                # Nettoyer le contenu pour extraire le JSON
                content = content.strip()
                if content.startswith('```json'):
                    content = content[7:-3]
                elif content.startswith('```'):
                    content = content[3:-3]
                
                import json
                plan_data = json.loads(content)
                
                # Validation des champs requis
                required_fields = ["analyse", "plan", "requetes_recherche", "questions_secondaires", "strategie"]
                if all(field in plan_data for field in required_fields):
                    # Limiter le nombre de requ√™tes
                    plan_data["requetes_recherche"] = plan_data["requetes_recherche"][:6]
                    
                    # Affichage d√©taill√© du plan dans la console avec couleurs
                    colored_log("info", "üìã ========= PLAN DE RECHERCHE INTELLIGENT =========", Fore.CYAN + Style.BRIGHT)
                    colored_log("info", f"üéØ ANALYSE: {plan_data['analyse']}", Fore.YELLOW + Style.BRIGHT)
                    colored_log("info", "üìä √âTAPES DU PLAN:", Fore.GREEN + Style.BRIGHT)
                    for i, etape in enumerate(plan_data['plan'], 1):
                        colored_log("info", f"   {i}. {etape}", Fore.GREEN)
                    colored_log("info", "üîç REQU√äTES DE RECHERCHE:", Fore.BLUE + Style.BRIGHT)
                    for i, query in enumerate(plan_data['requetes_recherche'], 1):
                        colored_log("info", f"   {i}. '{query}'", Fore.BLUE)
                    colored_log("info", "‚ùì QUESTIONS SECONDAIRES:", Fore.MAGENTA + Style.BRIGHT)
                    for question in plan_data['questions_secondaires']:
                        colored_log("info", f"   ‚Ä¢ {question}", Fore.MAGENTA)
                    colored_log("info", f"üé≤ STRAT√âGIE: {plan_data['strategie']}", Fore.WHITE + Style.BRIGHT)
                    colored_log("info", "=" * 55, Fore.CYAN + Style.BRIGHT)
                    
                    return {
                        "requetes_recherche": plan_data["requetes_recherche"],
                        "types_sources": ["donn√©es officielles", "sites sp√©cialis√©s", "√©tudes", "articles de r√©f√©rence"],
                        "questions_secondaires": plan_data["questions_secondaires"],
                        "strategie": f"Plan intelligent: {plan_data['strategie']}",
                        "analyse": plan_data["analyse"],
                        "plan_etapes": plan_data["plan"]
                    }
                    
            except (json.JSONDecodeError, KeyError) as e:
                logger.warning(f"‚ö†Ô∏è Erreur parsing JSON plan: {e}")
        
        # Fallback intelligent bas√© sur l'analyse de la question
        logger.info("üìã Plan de fallback intelligent")
        return self._generate_smart_fallback_plan(user_query)
    
    def _make_request_mistral_json(self, prompt: str, max_tokens: int = 800) -> Optional[str]:
        """Requ√™te Mistral avec format JSON forc√©"""
        if not self.config.MISTRAL_API_KEY:
            return None
            
        try:
            self._wait_for_rate_limit()
            
            payload = {
                "model": self.config.MISTRAL_MODEL,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": self.config.TEMPERATURE,
                "response_format": {"type": "json_object"}
            }
            
            logger.info("ü§ñ Requ√™te Mistral JSON")
            response = requests.post(
                "https://api.mistral.ai/v1/chat/completions",
                headers=self.mistral_headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                logger.info("‚úÖ Requ√™te Mistral JSON r√©ussie")
                return content
            elif response.status_code == 429:
                logger.warning("‚ö†Ô∏è Rate limit Mistral, attente...")
                time.sleep(5)
                return None
            else:
                logger.warning(f"‚ö†Ô∏è Erreur Mistral JSON: {response.status_code}")
                return None
                
        except Exception as e:
            logger.warning(f"‚ùå Erreur requ√™te Mistral JSON: {e}")
            return None
    
    def _generate_smart_fallback_plan(self, user_query: str) -> Dict:
        """G√©n√®re un plan de fallback intelligent bas√© sur l'analyse de mots-cl√©s"""
        query_lower = user_query.lower()
        
        # D√©tection du type de question
        if any(word in query_lower for word in ["qui est", "plus riche", "fortune", "patrimoine", "richesse"]):
            # Question de comparaison financi√®re
            base_terms = ["fortune", "patrimoine", "richesse", "net worth"]
            if "musk" in query_lower and "hollande" in query_lower:
                return {
                    "requetes_recherche": [
                        "Elon Musk fortune 2024 milliards",
                        "Fran√ßois Hollande patrimoine d√©claration",
                        "richest people world 2024 Forbes",
                        "pr√©sident France salaire patrimoine",
                        "Tesla SpaceX valeur Musk",
                        "comparaison fortune politiques milliardaires"
                    ],
                    "types_sources": ["Forbes", "sites financiers", "d√©clarations officielles"],
                    "questions_secondaires": ["Sources de revenus de chacun ?", "√âvolution dans le temps ?"],
                    "strategie": "Fallback intelligent: comparaison financi√®re",
                    "analyse": "Comparaison de patrimoine entre personnalit√©s publiques",
                    "plan_etapes": ["Recherche fortune Musk", "Recherche patrimoine Hollande", "Comparaison", "Contexte"]
                }
        
        elif any(word in query_lower for word in ["comment", "dresser", "√©duquer", "apprendre"]):
            # Question pratique/tutoriel
            return {
                "requetes_recherche": [
                    f"{user_query} guide",
                    f"{user_query} conseils experts",
                    f"{user_query} m√©thode √©tapes",
                    f"{user_query} erreurs √©viter",
                    f"{user_query} t√©moignages",
                    f"{user_query} 2024 techniques"
                ],
                "types_sources": ["guides pratiques", "sites sp√©cialis√©s", "forums"],
                "questions_secondaires": ["Quelles erreurs √©viter ?", "Combien de temps √ßa prend ?"],
                "strategie": "Fallback intelligent: guide pratique",
                "analyse": "Recherche de conseils et m√©thodes pratiques",
                "plan_etapes": ["M√©thodes de base", "Conseils experts", "T√©moignages", "Erreurs courantes"]
            }
        
        # Fallback g√©n√©ral
        base_query = user_query.strip()
        return {
            "requetes_recherche": [
                base_query,
                f"{base_query} guide complet",
                f"{base_query} conseils experts",
                f"{base_query} 2024 actualit√©s",
                f"{base_query} avantages inconv√©nients",
                f"{base_query} t√©moignages avis"
            ],
            "types_sources": ["articles de r√©f√©rence", "sites sp√©cialis√©s"],
            "questions_secondaires": ["Quels sont les points cl√©s ?", "Quelles sont les tendances ?"],
            "strategie": "Fallback g√©n√©ral intelligent",
            "analyse": f"Recherche d'informations compl√®tes sur: {base_query}",
            "plan_etapes": ["Informations g√©n√©rales", "Avis experts", "Actualit√©s", "Retours utilisateurs"]
        }
    
    def synthesize_results(self, query: str, search_results: List[Dict], scraped_articles: List[Dict]) -> str:
        """Synth√©tise les r√©sultats de recherche"""
        
        # Pr√©parer le contexte
        context = f"Question: {query}\n\n"
        context += "Sources trouv√©es:\n"
        
        for i, result in enumerate(search_results[:5], 1):
            context += f"{i}. {result.get('title', 'N/A')}\n"
            context += f"   {result.get('snippet', 'N/A')[:200]}...\n\n"
        
        if scraped_articles:
            context += "\nArticles analys√©s:\n"
            for i, article in enumerate(scraped_articles[:3], 1):
                context += f"{i}. {article.get('title', 'N/A')}\n"
                context += f"   {article.get('content', 'N/A')[:300]}...\n\n"
        
        prompt = f"""{context}

Bas√© sur ces informations, r√©dige une synth√®se claire et structur√©e r√©pondant √†: "{query}"

Structure:
- Introduction courte
- Points cl√©s (3-4 points)
- Conclusion

Reste factuel et cite les sources quand pertinent."""

        content = self.generate_completion(prompt, max_tokens=800)
        
        if content:
            return content
        else:
            # Fallback simple
            return f"""Synth√®se pour: {query}

Bas√© sur {len(search_results)} sources trouv√©es et {len(scraped_articles)} articles analys√©s, voici les informations principales:

{search_results[0].get('snippet', 'Informations non disponibles') if search_results else 'Aucune source trouv√©e'}

Note: Cette synth√®se a √©t√© g√©n√©r√©e automatiquement en raison d'un probl√®me temporaire avec l'IA.""" 