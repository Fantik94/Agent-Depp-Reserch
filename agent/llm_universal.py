import requests
import json
import time
import logging
from typing import List, Dict, Optional
from config import Config

logger = logging.getLogger(__name__)

class UniversalLLMClient:
    """Client LLM universel supportant Mistral, Groq et Ollama"""
    
    def __init__(self, provider: str = None):
        self.config = Config()
        self.provider = provider or self.config.LLM_PROVIDER
        self.last_request_time = 0
        self.min_delay = 1  # D√©lai r√©duit
        
        # Initialiser le client selon le provider
        if self.provider == "mistral":
            try:
                from mistralai.client import MistralClient
                from mistralai.models.chat_completion import ChatMessage
                self.client = MistralClient(api_key=self.config.MISTRAL_API_KEY)
                self.ChatMessage = ChatMessage
                logger.info(f"ü§ñ Client Mistral initialis√© avec {self.config.MISTRAL_MODEL}")
            except Exception as e:
                logger.error(f"‚ùå Erreur initialisation Mistral: {e}")
                logger.info("üîÑ Basculement vers Groq")
                self.provider = "groq"  # Fallback vers Groq
                self._init_groq()
        elif self.provider == "groq":
            self._init_groq()
        elif self.provider == "ollama":
            # Ollama local
            pass
        else:
            logger.warning(f"‚ö†Ô∏è Provider inconnu: {self.provider}, utilisation de Groq")
            self.provider = "groq"
            self._init_groq()
            
        logger.info(f"ü§ñ Client LLM initialis√©: {self.provider}")
    
    def _wait_for_rate_limit(self):
        """Attend avant la prochaine requ√™te (d√©lai r√©duit)"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_delay:
            wait_time = self.min_delay - time_since_last
            time.sleep(wait_time)
        
        self.last_request_time = time.time()
    
    def _make_request_groq(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """Requ√™te vers Groq API (gratuit, rapide) avec fallback automatique"""
        
        # Liste des mod√®les Groq gratuits √† tester
        groq_models = [
            "llama-3.3-70b-versatile",  # Plus r√©cent
            "llama-3.1-8b-instant",     # Plus rapide
            "mixtral-8x7b-32768",       # Bon contexte
            "gemma2-9b-it",             # Alternative
            "llama3-groq-70b-8192-tool-use-preview"  # Sp√©cialis√© tools
        ]
        
        for model in groq_models:
            try:
                self._wait_for_rate_limit()
                
                payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": max_tokens,
                    "temperature": self.config.TEMPERATURE
                }
                
                logger.info(f"üöÄ Requ√™te Groq (gratuit) - {model}")
                response = requests.post(
                    "https://api.groq.com/openai/v1/chat/completions",
                    headers=self.groq_headers,
                    json=payload,
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result["choices"][0]["message"]["content"]
                    logger.info(f"‚úÖ Requ√™te Groq r√©ussie avec {model}")
                    return content
                else:
                    logger.warning(f"‚ö†Ô∏è Groq {model} erreur {response.status_code}")
                    continue  # Essayer le mod√®le suivant
                    
            except Exception as e:
                logger.warning(f"‚ùå Erreur Groq {model}: {e}")
                continue  # Essayer le mod√®le suivant
        
        logger.error("‚ùå Tous les mod√®les Groq ont √©chou√©")
        return None
    
    def _make_request_ollama(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """Requ√™te vers Ollama local (gratuit)"""
        try:
            self._wait_for_rate_limit()
            
            payload = {
                "model": self.config.OLLAMA_MODEL,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": self.config.TEMPERATURE
                }
            }
            
            logger.info("üè† Requ√™te Ollama (local)")
            response = requests.post(
                f"{self.config.OLLAMA_BASE_URL}/api/generate",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get("response", "")
                logger.info("‚úÖ Requ√™te Ollama r√©ussie")
                return content
            else:
                logger.warning(f"‚ö†Ô∏è Ollama erreur {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erreur Ollama: {e}")
            return None
    
    def _make_request_mistral(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """Requ√™te vers Mistral (avec rate limiting am√©lior√©)"""
        try:
            # V√©rifier que Mistral est bien configur√©
            if not hasattr(self, 'client') or not hasattr(self, 'ChatMessage'):
                logger.error("‚ùå Client Mistral non initialis√©")
                return None
            
            # D√©lai plus long pour Mistral
            time.sleep(3)
            
            messages = [self.ChatMessage(role="user", content=prompt)]
            
            logger.info("ü§ñ Requ√™te Mistral")
            response = self.client.chat(
                model=self.config.MISTRAL_MODEL,
                messages=messages,
                max_tokens=max_tokens,
                temperature=self.config.TEMPERATURE
            )
            
            logger.info("‚úÖ Requ√™te Mistral r√©ussie")
            return response.choices[0].message.content
            
        except Exception as e:
            error_str = str(e)
            if "429" in error_str or "Too Many Requests" in error_str:
                logger.warning("‚ö†Ô∏è Rate limit Mistral - basculement vers Groq")
                return self._make_request_groq(prompt, max_tokens)
            else:
                logger.error(f"‚ùå Erreur Mistral: {e}")
                return None
    
    def generate_completion(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """G√©n√®re une completion avec le provider configur√©"""
        
        if self.provider == "groq":
            return self._make_request_groq(prompt, max_tokens)
        elif self.provider == "ollama":
            return self._make_request_ollama(prompt, max_tokens)
        elif self.provider == "mistral":
            return self._make_request_mistral(prompt, max_tokens)
        else:
            logger.error(f"‚ùå Provider non support√©: {self.provider}")
            return None
    
    def generate_search_plan(self, user_query: str) -> Dict:
        """G√©n√®re un plan de recherche standard"""
        
        # Plan simple sans LLM si la requ√™te est courte (gain de temps)
        if len(user_query.split()) <= 3:
            logger.info("üìã Plan simple g√©n√©r√© (sans LLM)")
            base_query = user_query.strip()
            return {
                "requetes_recherche": [
                    base_query,
                    f"{base_query} avantages",
                    f"{base_query} inconv√©nients"
                ],
                "types_sources": ["articles sp√©cialis√©s", "√©tudes", "sites d'information"],
                "questions_secondaires": ["Quels sont les aspects importants ?"],
                "strategie": "Plan simple g√©n√©r√© automatiquement"
            }
        
        prompt = f"""Cr√©e 3 requ√™tes de recherche courtes pour: "{user_query}"

R√©ponds UNIQUEMENT avec les requ√™tes s√©par√©es par des virgules:
requ√™te1, requ√™te2, requ√™te3

Exemple pour "intelligence artificielle":
intelligence artificielle d√©finition, IA avantages inconv√©nients, intelligence artificielle applications"""

        content = self.generate_completion(prompt, max_tokens=150)
        
        if content:
            # Extraire les requ√™tes
            queries = [q.strip() for q in content.split(',') if q.strip()]
            
            if len(queries) >= 2:
                logger.info(f"üìã Plan LLM g√©n√©r√© avec {len(queries)} requ√™tes")
                return {
                    "requetes_recherche": queries[:4],  # Max 4 pour la vitesse
                    "types_sources": ["articles sp√©cialis√©s", "√©tudes", "sites d'information"],
                    "questions_secondaires": ["Quels sont les aspects importants ?"],
                    "strategie": f"Plan g√©n√©r√© par {self.provider.upper()}"
                }
        
        # Fallback simple
        logger.info("üìã Plan automatique g√©n√©r√©")
        base_query = user_query.strip()
        return {
            "requetes_recherche": [
                base_query,
                f"{base_query} 2024",
                f"{base_query} avantages"
            ],
            "types_sources": ["articles", "sites web"],
            "questions_secondaires": [],
            "strategie": "Plan automatique"
        }
    
    def generate_deep_search_plan(self, user_query: str) -> Dict:
        """G√©n√®re un plan de recherche approfondi"""
        
        prompt = f"""Cr√©e un plan de recherche approfondi pour: "{user_query}"

G√©n√®re 5-6 requ√™tes vari√©es:
- G√©n√©ral
- Avantages/inconv√©nients  
- Tendances 2024
- Experts/√©tudes
- Applications pratiques

R√©ponds UNIQUEMENT avec les requ√™tes s√©par√©es par des virgules.

Exemple pour "intelligence artificielle":
intelligence artificielle d√©finition, IA avantages inconv√©nients, intelligence artificielle 2024, IA experts avis, intelligence artificielle applications pratiques"""

        content = self.generate_completion(prompt, max_tokens=200)
        
        if content:
            queries = [q.strip() for q in content.split(',') if q.strip()]
            
            if len(queries) >= 3:
                logger.info(f"üìã Plan approfondi g√©n√©r√© avec {len(queries)} requ√™tes")
                return {
                    "requetes_recherche": queries[:6],
                    "types_sources": ["articles sp√©cialis√©s", "√©tudes", "sites d'actualit√©", "blogs experts"],
                    "questions_secondaires": ["Quels sont les enjeux ?", "Quelles perspectives ?"],
                    "strategie": f"Plan approfondi g√©n√©r√© par {self.provider.upper()}"
                }
        
        # Fallback approfondi
        logger.info("üìã Plan approfondi automatique")
        base_query = user_query.strip()
        return {
            "requetes_recherche": [
                base_query,
                f"{base_query} avantages",
                f"{base_query} inconv√©nients", 
                f"{base_query} 2024",
                f"{base_query} tendances",
                f"{base_query} experts"
            ],
            "types_sources": ["articles", "√©tudes", "blogs"],
            "questions_secondaires": ["Quels sont les aspects ?"],
            "strategie": "Plan approfondi automatique"
        }
    
    def synthesize_results(self, query: str, search_results: List[Dict], scraped_articles: List[Dict]) -> str:
        """Synth√©tise les r√©sultats de recherche"""
        
        # Pr√©parer le contexte
        context = f"Question: {query}\n\n"
        context += "Sources trouv√©es:\n"
        
        for i, result in enumerate(search_results[:5], 1):
            context += f"{i}. {result.get('title', 'N/A')}\n"
            context += f"   {result.get('snippet', 'N/A')[:200]}...\n\n"
        
        if scraped_articles:
            context += "\nArticles analys√©s:\n"
            for i, article in enumerate(scraped_articles[:3], 1):
                context += f"{i}. {article.get('title', 'N/A')}\n"
                context += f"   {article.get('content', 'N/A')[:300]}...\n\n"
        
        prompt = f"""{context}

Bas√© sur ces informations, r√©dige une synth√®se claire et structur√©e r√©pondant √†: "{query}"

Structure:
- Introduction courte
- Points cl√©s (3-4 points)
- Conclusion

Reste factuel et cite les sources quand pertinent."""

        content = self.generate_completion(prompt, max_tokens=800)
        
        if content:
            return content
        else:
            # Fallback simple
            return f"""Synth√®se pour: {query}

Bas√© sur {len(search_results)} sources trouv√©es et {len(scraped_articles)} articles analys√©s, voici les informations principales:

{search_results[0].get('snippet', 'Informations non disponibles') if search_results else 'Aucune source trouv√©e'}

Note: Cette synth√®se a √©t√© g√©n√©r√©e automatiquement en raison d'un probl√®me temporaire avec l'IA.""" 