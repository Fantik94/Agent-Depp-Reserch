from typing import Dict, List
import logging
from search_api import SearchAPI
from scraper import WebScraper
from llm_universal import UniversalLLMClient
from config import Config

logger = logging.getLogger(__name__)

class ResearchAgent:
    """Agent de recherche principal qui coordonne toutes les Ã©tapes"""
    
    def __init__(self, llm_provider: str = None, search_engines: List[str] = None, scraping_method: str = "both"):
        self.config = Config()
        self.search_api = SearchAPI()
        self.scraper = WebScraper()
        self.llm_client = UniversalLLMClient(provider=llm_provider)
        self.search_engines = search_engines
        self.scraping_method = scraping_method
    
    def research(self, user_query: str) -> Dict:
        """Effectue une recherche complÃ¨te basÃ©e sur la requÃªte utilisateur"""
        
        logger.info(f"DÃ©but de la recherche pour: {user_query}")
        
        # Ã‰tape 1: GÃ©nÃ©ration du plan de recherche
        logger.info("ğŸ“‹ GÃ©nÃ©ration du plan de recherche...")
        plan = self.llm_client.generate_search_plan(user_query)
        
        # Ã‰tape 2: Recherche web pour chaque requÃªte du plan
        logger.info("ğŸ” Recherche web en cours...")
        all_search_results = []
        
        for query in plan.get("requetes_recherche", [user_query]):
            results = self.search_api.search_web(query)
            all_search_results.extend(results)
            logger.info(f"  - '{query}': {len(results)} rÃ©sultats")
        
        # Supprimer les doublons
        unique_results = []
        seen_urls = set()
        for result in all_search_results:
            if result['url'] not in seen_urls:
                seen_urls.add(result['url'])
                unique_results.append(result)
        
        # Ã‰tape 3: Scraping des articles les plus pertinents
        logger.info("ğŸ“° Scraping des articles...")
        urls_to_scrape = [result['url'] for result in unique_results[:self.config.MAX_SCRAPED_ARTICLES * 2]]
        scraped_articles = self.scraper.scrape_multiple_urls(urls_to_scrape)
        
        # Ã‰tape 4: SynthÃ¨se des rÃ©sultats
        logger.info("âœï¸ SynthÃ¨se des rÃ©sultats...")
        synthesis = self.llm_client.synthesize_results(user_query, unique_results, scraped_articles)
        
        # PrÃ©parer le rÃ©sultat final
        result = {
            "query": user_query,
            "plan": plan,
            "search_results": unique_results,
            "scraped_articles": scraped_articles,
            "synthesis": synthesis,
            "stats": {
                "search_results_count": len(unique_results),
                "scraped_articles_count": len(scraped_articles),
                "search_queries_used": len(plan.get("requetes_recherche", []))
            }
        }
        
        logger.info("âœ… Recherche terminÃ©e avec succÃ¨s")
        return result
    
    def quick_search(self, query: str) -> str:
        """Recherche rapide qui retourne directement la synthÃ¨se"""
        result = self.research(query)
        return result["synthesis"] 