from mistralai import Mistral
from mistralai.models import UserMessage
from typing import List, Dict
import logging
from config import Config
import json
import re
import time
import random

logger = logging.getLogger(__name__)

class MistralLLMClient:
    """Client Mistral pour la planification et synth√®se avec gestion du rate limiting"""
    
    def __init__(self):
        self.config = Config()
        self.client = Mistral(api_key=self.config.MISTRAL_API_KEY)
        self.last_request_time = 0
        self.min_delay = 5  # D√©lai minimum entre requ√™tes (secondes) - augment√© pour Mistral
    
    def _wait_for_rate_limit(self):
        """Attend avant la prochaine requ√™te pour respecter le rate limit"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_delay:
            wait_time = self.min_delay - time_since_last
            logger.info(f"‚è≥ Attente {wait_time:.1f}s pour respecter le rate limit")
            time.sleep(wait_time)
        
        self.last_request_time = time.time()
    
    def _make_request_with_retry(self, messages: List[UserMessage], max_tokens: int = 500, temperature: float = 0.3, max_retries: int = 3):
        """Effectue une requ√™te avec retry automatique en cas d'erreur 429"""
        
        for attempt in range(max_retries):
            try:
                self._wait_for_rate_limit()
                
                logger.info(f"ü§ñ Requ√™te Mistral (tentative {attempt + 1}/{max_retries})")
                
                response = self.client.chat.complete(
                    model=self.config.MISTRAL_MODEL,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                logger.info("‚úÖ Requ√™te Mistral r√©ussie")
                return response.choices[0].message.content
                
            except Exception as e:
                error_str = str(e)
                
                if "429" in error_str or "Too Many Requests" in error_str:
                    wait_time = (2 ** attempt) * 10  # Backoff exponentiel: 10s, 20s, 40s
                    logger.warning(f"‚ö†Ô∏è Rate limit atteint, attente {wait_time}s...")
                    time.sleep(wait_time)
                    
                    if attempt == max_retries - 1:
                        logger.error("‚ùå √âchec apr√®s tous les retries, utilisation du fallback")
                        return None
                else:
                    logger.error(f"‚ùå Erreur Mistral: {e}")
                    return None
        
        return None
    
    def generate_deep_search_plan(self, user_query: str) -> Dict:
        """G√©n√®re un plan de recherche approfondi avec plus de requ√™tes"""
        
        prompt = f"""Cr√©e un plan de recherche approfondi pour: "{user_query}"

G√©n√®re 5-7 requ√™tes de recherche vari√©es pour explorer tous les aspects:
- Requ√™tes g√©n√©rales
- Requ√™tes sp√©cialis√©es 
- Requ√™tes sur les avantages/inconv√©nients
- Requ√™tes sur les tendances r√©centes
- Requ√™tes d'experts/√©tudes

R√©ponds UNIQUEMENT avec les requ√™tes s√©par√©es par des virgules:
requ√™te1, requ√™te2, requ√™te3, requ√™te4, requ√™te5, requ√™te6, requ√™te7

Exemple pour "intelligence artificielle":
intelligence artificielle d√©finition, IA avantages inconv√©nients, intelligence artificielle applications 2024, IA √©thique risques, intelligence artificielle emploi impact, IA experts opinions, intelligence artificielle futur tendances"""

        content = self._make_request_with_retry(
            [UserMessage(content=prompt)],
            max_tokens=200,
            temperature=0.5
        )
        
        if content:
            # Extraire les requ√™tes
            queries = [q.strip() for q in content.split(',') if q.strip()]
            
            # Validation et nettoyage
            if len(queries) >= 3:
                logger.info(f"üìã Plan approfondi g√©n√©r√© avec {len(queries)} requ√™tes")
                return {
                    "requetes_recherche": queries[:7],  # Max 7 requ√™tes
                    "types_sources": ["articles sp√©cialis√©s", "√©tudes acad√©miques", "sites d'actualit√©", "blogs d'experts"],
                    "questions_secondaires": ["Quels sont les enjeux actuels ?", "Quelles sont les perspectives d'avenir ?"],
                    "strategie": "Plan approfondi g√©n√©r√© par LLM"
                }
        
        # Fallback: plan approfondi manuel
        logger.info("üìã G√©n√©ration de plan approfondi automatique")
        base_query = user_query.strip()
        return {
            "requetes_recherche": [
                base_query,
                f"{base_query} avantages",
                f"{base_query} inconv√©nients",
                f"{base_query} 2024",
                f"{base_query} tendances",
                f"{base_query} experts avis",
                f"{base_query} futur"
            ],
            "types_sources": ["articles sp√©cialis√©s", "√©tudes", "sites d'information", "blogs experts"],
            "questions_secondaires": ["Quels sont les aspects importants ?", "Quelles sont les tendances ?"],
            "strategie": "Plan approfondi automatique"
        }
    
    def generate_search_plan(self, user_query: str) -> Dict:
        """G√©n√®re un plan de recherche bas√© sur la requ√™te utilisateur"""
        
        # Plan simple sans LLM si la requ√™te est courte
        if len(user_query.split()) <= 3:
            logger.info("üìã G√©n√©ration de plan simple (sans LLM)")
            base_query = user_query.strip()
            return {
                "requetes_recherche": [
                    base_query,
                    f"{base_query} avantages",
                    f"{base_query} inconv√©nients"
                ],
                "types_sources": ["articles sp√©cialis√©s", "√©tudes", "sites d'information"],
                "questions_secondaires": ["Quels sont les aspects importants ?"],
                "strategie": "Plan simple g√©n√©r√© automatiquement"
            }
        
        prompt = f"""Cr√©e 3 requ√™tes de recherche courtes pour: "{user_query}"

R√©ponds UNIQUEMENT avec les requ√™tes s√©par√©es par des virgules:
requ√™te1, requ√™te2, requ√™te3

Exemple pour "intelligence artificielle":
intelligence artificielle d√©finition, IA avantages inconv√©nients, intelligence artificielle applications"""

        content = self._make_request_with_retry(
            [UserMessage(content=prompt)],
            max_tokens=150,
            temperature=0.3
        )
        
        if content:
            # Parser les requ√™tes
            queries = [q.strip() for q in content.split(',') if q.strip()]
            
            if queries and len(queries) >= 2:
                plan = {
                    "requetes_recherche": queries[:4],  # Max 4 requ√™tes
                    "types_sources": ["articles sp√©cialis√©s", "√©tudes", "sites d'information"],
                    "questions_secondaires": ["Quels sont les aspects importants ?"],
                    "strategie": f"Plan LLM avec {len(queries)} requ√™tes"
                }
                
                logger.info(f"üìã Plan LLM g√©n√©r√© avec {len(queries)} requ√™tes")
                return plan
        
        # Fallback si LLM √©choue
        logger.warning("‚ö†Ô∏è Fallback: g√©n√©ration de plan simple")
        base_terms = user_query.split()[-3:]  # 3 derniers mots
        base_query = " ".join(base_terms)
        
        return {
            "requetes_recherche": [
                base_query,
                f"{base_query} d√©finition",
                f"{base_query} avantages"
            ],
            "types_sources": ["tous types"],
            "questions_secondaires": [],
            "strategie": "Plan de fallback (LLM indisponible)"
        }
    
    def synthesize_results(self, user_query: str, search_results: List[Dict], scraped_articles: List[Dict]) -> str:
        """Synth√©tise les r√©sultats de recherche et articles scrap√©s"""
        
        # Si pas de donn√©es, retourner directement
        if not search_results and not scraped_articles:
            return f"""**Aucune information trouv√©e pour : "{user_query}"**

Cela peut √™tre d√ª √† :
- Des termes de recherche trop sp√©cifiques
- Des restrictions d'acc√®s aux sites web
- Des probl√®mes de connectivit√©

üí° **Suggestion :** Essayez avec des termes plus g√©n√©raux ou reformulez votre question."""

        # Pr√©parer un contexte tr√®s concis pour √©viter les tokens excessifs
        context_parts = []
        
        if search_results:
            context_parts.append("**Sources trouv√©es :**")
            for i, result in enumerate(search_results[:3], 1):
                context_parts.append(f"{i}. {result['title']}: {result['snippet'][:100]}...")
        
        if scraped_articles:
            context_parts.append("\n**Articles analys√©s :**")
            for i, article in enumerate(scraped_articles[:2], 1):
                context_parts.append(f"{i}. {article['title']}: {article['content'][:200]}...")
        
        context = "\n".join(context_parts)
        
        # Requ√™te tr√®s simple pour √©conomiser les tokens
        prompt = f"""Question: {user_query}

{context}

Fais une synth√®se en fran√ßais, courte et claire."""

        content = self._make_request_with_retry(
            [UserMessage(content=prompt)],
            max_tokens=800,
            temperature=0.5
        )
        
        if content:
            # Ajouter les sources
            synthesis = content
            
            if search_results:
                synthesis += f"\n\n**üìö Sources consult√©es :**"
                for i, result in enumerate(search_results[:3], 1):
                    synthesis += f"\n{i}. {result['title']} - {result['url']}"
            
            return synthesis
        
        # Fallback manuel si LLM √©choue
        logger.warning("‚ö†Ô∏è Synth√®se manuelle (LLM indisponible)")
        
        fallback = f"**R√©sultats de recherche pour : {user_query}**\n\n"
        
        if search_results:
            fallback += "**üîç Sources trouv√©es :**\n"
            for i, result in enumerate(search_results[:5], 1):
                fallback += f"{i}. **{result['title']}**\n"
                fallback += f"   {result['snippet'][:200]}...\n"
                fallback += f"   üîó {result['url']}\n\n"
        
        if scraped_articles:
            fallback += "**üì∞ Articles analys√©s :**\n"
            for i, article in enumerate(scraped_articles[:3], 1):
                fallback += f"{i}. **{article['title']}**\n"
                fallback += f"   {article['content'][:300]}...\n"
                fallback += f"   üîó {article['url']}\n\n"
        
        fallback += "*Note : Synth√®se automatique temporairement indisponible (rate limit API)*"
        
        return fallback 