from mistralai import Mistral
from mistralai.models import UserMessage
from typing import List, Dict
import logging
from config import Config
import json
import re
import time
import random

logger = logging.getLogger(__name__)

class MistralLLMClient:
    """Client Mistral pour la planification et synthÃ¨se avec gestion du rate limiting"""
    
    def __init__(self):
        self.config = Config()
        self.client = Mistral(api_key=self.config.MISTRAL_API_KEY)
        self.last_request_time = 0
        self.min_delay = 5  # DÃ©lai minimum entre requÃªtes (secondes) - augmentÃ© pour Mistral
    
    def _wait_for_rate_limit(self):
        """Attend avant la prochaine requÃªte pour respecter le rate limit"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_delay:
            wait_time = self.min_delay - time_since_last
            logger.info(f"â³ Attente {wait_time:.1f}s pour respecter le rate limit")
            time.sleep(wait_time)
        
        self.last_request_time = time.time()
    
    def _make_request_with_retry(self, messages: List[UserMessage], max_tokens: int = 500, temperature: float = 0.3, max_retries: int = 3):
        """Effectue une requÃªte avec retry automatique en cas d'erreur 429"""
        
        for attempt in range(max_retries):
            try:
                self._wait_for_rate_limit()
                
                logger.info(f"ğŸ¤– RequÃªte Mistral (tentative {attempt + 1}/{max_retries})")
                
                response = self.client.chat.complete(
                    model=self.config.MISTRAL_MODEL,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                logger.info("âœ… RequÃªte Mistral rÃ©ussie")
                return response.choices[0].message.content
                
            except Exception as e:
                error_str = str(e)
                
                if "429" in error_str or "Too Many Requests" in error_str:
                    wait_time = (2 ** attempt) * 10  # Backoff exponentiel: 10s, 20s, 40s
                    logger.warning(f"âš ï¸ Rate limit atteint, attente {wait_time}s...")
                    time.sleep(wait_time)
                    
                    if attempt == max_retries - 1:
                        logger.error("âŒ Ã‰chec aprÃ¨s tous les retries, utilisation du fallback")
                        return None
                else:
                    logger.error(f"âŒ Erreur Mistral: {e}")
                    return None
        
        return None
    
    def generate_deep_search_plan(self, user_query: str) -> Dict:
        """GÃ©nÃ¨re un plan de recherche approfondi avec plus de requÃªtes"""
        
        prompt = f"""CrÃ©e un plan de recherche approfondi pour: "{user_query}"

GÃ©nÃ¨re 5-7 requÃªtes de recherche variÃ©es pour explorer tous les aspects:
- RequÃªtes gÃ©nÃ©rales
- RequÃªtes spÃ©cialisÃ©es 
- RequÃªtes sur les avantages/inconvÃ©nients
- RequÃªtes sur les tendances rÃ©centes
- RequÃªtes d'experts/Ã©tudes

RÃ©ponds UNIQUEMENT avec les requÃªtes sÃ©parÃ©es par des virgules:
requÃªte1, requÃªte2, requÃªte3, requÃªte4, requÃªte5, requÃªte6, requÃªte7

Exemple pour "intelligence artificielle":
intelligence artificielle dÃ©finition, IA avantages inconvÃ©nients, intelligence artificielle applications 2024, IA Ã©thique risques, intelligence artificielle emploi impact, IA experts opinions, intelligence artificielle futur tendances"""

        content = self._make_request_with_retry(
            [UserMessage(content=prompt)],
            max_tokens=200,
            temperature=0.5
        )
        
        if content:
            # Extraire les requÃªtes
            queries = [q.strip() for q in content.split(',') if q.strip()]
            
            # Validation et nettoyage
            if len(queries) >= 3:
                logger.info(f"ğŸ“‹ Plan approfondi gÃ©nÃ©rÃ© avec {len(queries)} requÃªtes")
                return {
                    "requetes_recherche": queries[:7],  # Max 7 requÃªtes
                    "types_sources": ["articles spÃ©cialisÃ©s", "Ã©tudes acadÃ©miques", "sites d'actualitÃ©", "blogs d'experts"],
                    "questions_secondaires": ["Quels sont les enjeux actuels ?", "Quelles sont les perspectives d'avenir ?"],
                    "strategie": "Plan approfondi gÃ©nÃ©rÃ© par LLM"
                }
        
        # Fallback: plan approfondi manuel
        logger.info("ğŸ“‹ GÃ©nÃ©ration de plan approfondi automatique")
        base_query = user_query.strip()
        return {
            "requetes_recherche": [
                base_query,
                f"{base_query} avantages",
                f"{base_query} inconvÃ©nients",
                f"{base_query} 2024",
                f"{base_query} tendances",
                f"{base_query} experts avis",
                f"{base_query} futur"
            ],
            "types_sources": ["articles spÃ©cialisÃ©s", "Ã©tudes", "sites d'information", "blogs experts"],
            "questions_secondaires": ["Quels sont les aspects importants ?", "Quelles sont les tendances ?"],
            "strategie": "Plan approfondi automatique"
        }
    
    def generate_search_plan(self, user_query: str) -> Dict:
        """GÃ©nÃ¨re un plan de recherche basÃ© sur la requÃªte utilisateur"""
        
        # Plan simple sans LLM si la requÃªte est courte
        if len(user_query.split()) <= 3:
            logger.info("ğŸ“‹ GÃ©nÃ©ration de plan simple (sans LLM)")
            base_query = user_query.strip()
            return {
                "requetes_recherche": [
                    base_query,
                    f"{base_query} avantages",
                    f"{base_query} inconvÃ©nients"
                ],
                "types_sources": ["articles spÃ©cialisÃ©s", "Ã©tudes", "sites d'information"],
                "questions_secondaires": ["Quels sont les aspects importants ?"],
                "strategie": "Plan simple gÃ©nÃ©rÃ© automatiquement"
            }
        
        prompt = f"""CrÃ©e 3 requÃªtes de recherche courtes pour: "{user_query}"

RÃ©ponds UNIQUEMENT avec les requÃªtes sÃ©parÃ©es par des virgules:
requÃªte1, requÃªte2, requÃªte3

Exemple pour "intelligence artificielle":
intelligence artificielle dÃ©finition, IA avantages inconvÃ©nients, intelligence artificielle applications"""

        content = self._make_request_with_retry(
            [UserMessage(content=prompt)],
            max_tokens=150,
            temperature=0.3
        )
        
        if content:
            # Parser les requÃªtes
            queries = [q.strip() for q in content.split(',') if q.strip()]
            
            if queries and len(queries) >= 2:
                plan = {
                    "requetes_recherche": queries[:4],  # Max 4 requÃªtes
                    "types_sources": ["articles spÃ©cialisÃ©s", "Ã©tudes", "sites d'information"],
                    "questions_secondaires": ["Quels sont les aspects importants ?"],
                    "strategie": f"Plan LLM avec {len(queries)} requÃªtes"
                }
                
                logger.info(f"ğŸ“‹ Plan LLM gÃ©nÃ©rÃ© avec {len(queries)} requÃªtes")
                return plan
        
        # Fallback si LLM Ã©choue
        logger.warning("âš ï¸ Fallback: gÃ©nÃ©ration de plan simple")
        base_terms = user_query.split()[-3:]  # 3 derniers mots
        base_query = " ".join(base_terms)
        
        return {
            "requetes_recherche": [
                base_query,
                f"{base_query} dÃ©finition",
                f"{base_query} avantages"
            ],
            "types_sources": ["tous types"],
            "questions_secondaires": [],
            "strategie": "Plan de fallback (LLM indisponible)"
        }
    
    def synthesize_results(self, user_query: str, search_results: List[Dict], scraped_articles: List[Dict]) -> str:
        """SynthÃ©tise les rÃ©sultats de recherche et articles scrapÃ©s"""
        
        # Si pas de donnÃ©es, retourner directement
        if not search_results and not scraped_articles:
            return f"""**Aucune information trouvÃ©e pour : "{user_query}"**

Cela peut Ãªtre dÃ» Ã  :
- Des termes de recherche trop spÃ©cifiques
- Des restrictions d'accÃ¨s aux sites web
- Des problÃ¨mes de connectivitÃ©

ğŸ’¡ **Suggestion :** Essayez avec des termes plus gÃ©nÃ©raux ou reformulez votre question."""

        # PrÃ©parer un contexte trÃ¨s concis pour Ã©viter les tokens excessifs
        context_parts = []
        
        if search_results:
            context_parts.append("**Sources trouvÃ©es :**")
            for i, result in enumerate(search_results[:3], 1):
                context_parts.append(f"{i}. {result['title']}: {result['snippet'][:100]}...")
        
        if scraped_articles:
            context_parts.append("\n**Articles analysÃ©s :**")
            for i, article in enumerate(scraped_articles[:2], 1):
                context_parts.append(f"{i}. {article['title']}: {article['content'][:200]}...")
        
        context = "\n".join(context_parts)
        
        # RequÃªte trÃ¨s simple pour Ã©conomiser les tokens
        prompt = f"""Question: {user_query}

{context}

Fais une synthÃ¨se en franÃ§ais, courte et claire."""

        content = self._make_request_with_retry(
            [UserMessage(content=prompt)],
            max_tokens=800,
            temperature=0.5
        )
        
        if content:
            # Ajouter les sources
            synthesis = content
            
            if search_results:
                synthesis += f"\n\n**ğŸ“š Sources consultÃ©es :**"
                for i, result in enumerate(search_results[:3], 1):
                    synthesis += f"\n{i}. {result['title']} - {result['url']}"
            
            return synthesis
        
        # Fallback manuel si LLM Ã©choue
        logger.warning("âš ï¸ SynthÃ¨se manuelle (LLM indisponible)")
        
        fallback = f"**RÃ©sultats de recherche pour : {user_query}**\n\n"
        
        if search_results:
            fallback += "**ğŸ” Sources trouvÃ©es :**\n"
            for i, result in enumerate(search_results[:5], 1):
                fallback += f"{i}. **{result['title']}**\n"
                fallback += f"   {result['snippet'][:200]}...\n"
                fallback += f"   ğŸ”— {result['url']}\n\n"
        
        if scraped_articles:
            fallback += "**ğŸ“° Articles analysÃ©s :**\n"
            for i, article in enumerate(scraped_articles[:3], 1):
                fallback += f"{i}. **{article['title']}**\n"
                fallback += f"   {article['content'][:300]}...\n"
                fallback += f"   ğŸ”— {article['url']}\n\n"
        
        fallback += "*Note : SynthÃ¨se automatique temporairement indisponible (rate limit API)*"
        
        return fallback 