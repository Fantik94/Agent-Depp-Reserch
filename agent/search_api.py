import requests
import json
from typing import List, Dict, Optional
from config import Config
import logging
import urllib.parse
import time
import random
from html_search import search_with_html

logger = logging.getLogger(__name__)

class SearchAPI:
    """API de recherche utilisant SerpApi et des m√©thodes alternatives"""
    
    def __init__(self):
        self.config = Config()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
    
    def search_serpapi_free(self, query: str, max_results: int = 10) -> List[Dict]:
        """Utilise SerpApi avec cl√© API personnelle"""
        try:
            # SerpApi avec cl√© API personnelle
            url = "https://serpapi.com/search.json"
            params = {
                'q': query,
                'engine': 'google',
                'num': max_results,
                'api_key': self.config.SERP_API_KEY
            }
            
            logger.info(f"üîç Tentative SerpApi pour: {query}")
            response = self.session.get(url, params=params, timeout=self.config.REQUEST_TIMEOUT)
            
            if response.status_code == 200:
                data = response.json()
                results = []
                
                if 'organic_results' in data:
                    for item in data['organic_results'][:max_results]:
                        results.append({
                            'title': item.get('title', ''),
                            'url': item.get('link', ''),
                            'snippet': item.get('snippet', ''),
                            'source': 'serpapi'
                        })
                
                logger.info(f"‚úÖ SerpApi: {len(results)} r√©sultats trouv√©s")
                return results
            else:
                logger.warning(f"‚ö†Ô∏è SerpApi erreur {response.status_code}: {response.text[:200]}")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur SerpApi: {e}")
        
        return []
    
    def search_searxng(self, query: str, max_results: int = 10) -> List[Dict]:
        """Utilise une instance publique de SearXNG"""
        try:
            # Instance publique de SearXNG
            url = "https://search.bus-hit.me/search"
            params = {
                'q': query,
                'format': 'json',
                'engines': 'google,bing',
                'categories': 'general'
            }
            
            logger.info(f"üîç Tentative SearXNG pour: {query}")
            response = self.session.get(url, params=params, timeout=self.config.REQUEST_TIMEOUT)
            
            if response.status_code == 200:
                data = response.json()
                results = []
                
                if 'results' in data:
                    for item in data['results'][:max_results]:
                        if item.get('url', '').startswith('http'):
                            results.append({
                                'title': item.get('title', ''),
                                'url': item.get('url', ''),
                                'snippet': item.get('content', ''),
                                'source': 'searxng'
                            })
                
                logger.info(f"‚úÖ SearXNG: {len(results)} r√©sultats trouv√©s")
                return results
            else:
                logger.warning(f"‚ö†Ô∏è SearXNG erreur {response.status_code}")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur SearXNG: {e}")
        
        return []
    
    def search_html_google(self, query: str, max_results: int = 10) -> List[Dict]:
        """Recherche Google avec HTML (remplace DuckDuckGo)"""
        try:
            logger.info(f"üîç Tentative Google HTML pour: {query}")
            results = search_with_html(query, max_results, engine="google")
            logger.info(f"‚úÖ Google HTML: {len(results)} r√©sultats trouv√©s")
            return results
        except Exception as e:
            logger.error(f"‚ùå Erreur Google HTML: {e}")
            return []
    
    def search_html_bing(self, query: str, max_results: int = 10) -> List[Dict]:
        """Recherche Bing avec HTML comme alternative"""
        try:
            logger.info(f"üîç Tentative Bing HTML pour: {query}")
            results = search_with_html(query, max_results, engine="bing")
            logger.info(f"‚úÖ Bing HTML: {len(results)} r√©sultats trouv√©s")
            return results
        except Exception as e:
            logger.error(f"‚ùå Erreur Bing HTML: {e}")
            return []
    
    def search_html_startpage(self, query: str, max_results: int = 10) -> List[Dict]:
        """Recherche Startpage avec HTML (proxy Google anonyme)"""
        try:
            logger.info(f"üîç Tentative Startpage HTML pour: {query}")
            results = search_with_html(query, max_results, engine="startpage")
            logger.info(f"‚úÖ Startpage HTML: {len(results)} r√©sultats trouv√©s")
            return results
        except Exception as e:
            logger.error(f"‚ùå Erreur Startpage HTML: {e}")
            return []
    
    def search_html_duckduckgo(self, query: str, max_results: int = 10) -> List[Dict]:
        """Recherche DuckDuckGo avec HTML"""
        try:
            logger.info(f"üîç Tentative DuckDuckGo HTML pour: {query}")
            results = search_with_html(query, max_results, engine="duckduckgo")
            logger.info(f"‚úÖ DuckDuckGo HTML: {len(results)} r√©sultats trouv√©s")
            return results
        except Exception as e:
            logger.error(f"‚ùå Erreur DuckDuckGo HTML: {e}")
            return []
    
    def search_serper(self, query: str, max_results: int = 10) -> List[Dict]:
        """Recherche avec Serper.dev API"""
        if not self.config.SERPER_API_KEY:
            logger.debug("üîë Pas de cl√© Serper configur√©e")
            return []
        
        try:
            headers = {
                'X-API-KEY': self.config.SERPER_API_KEY,
                'Content-Type': 'application/json'
            }
            
            payload = {
                'q': query,
                'num': max_results
            }
            
            logger.info(f"üîç Tentative Serper pour: {query}")
            response = requests.post(
                self.config.SERPER_URL, 
                headers=headers, 
                json=payload,
                timeout=self.config.REQUEST_TIMEOUT
            )
            
            if response.status_code == 200:
                data = response.json()
                results = []
                
                if 'organic' in data:
                    for item in data['organic']:
                        results.append({
                            'title': item.get('title', ''),
                            'url': item.get('link', ''),
                            'snippet': item.get('snippet', ''),
                            'source': 'serper'
                        })
                
                logger.info(f"‚úÖ Serper: {len(results)} r√©sultats trouv√©s")
                return results
            else:
                logger.warning(f"‚ö†Ô∏è Serper erreur {response.status_code}: {response.text[:200]}")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur Serper: {e}")
        
        return []
    
    def create_fallback_results(self, query: str) -> List[Dict]:
        """Cr√©e des r√©sultats de fallback bas√©s sur des sites populaires avec URLs valides"""
        logger.info(f"üÜò Cr√©ation de r√©sultats de fallback pour: {query}")
        
        # Mots-cl√©s pour cat√©goriser la recherche
        query_lower = query.lower()
        
        results = []
        
        # Fallback intelligent selon le type de requ√™te
        if any(word in query_lower for word in ['sant√©', 'maladie', 'sympt√¥me', 'm√©dical', 'docteur']):
            results.extend([
                {
                    'title': f"Information m√©dicale sur {query}",
                    'url': "https://www.ameli.fr/",
                    'snippet': f"Informations fiables sur {query} - Site officiel de l'Assurance Maladie",
                    'source': 'fallback'
                },
                {
                    'title': f"Conseils sant√© : {query}",
                    'url': "https://www.doctissimo.fr/",
                    'snippet': f"Conseils et informations sant√© concernant {query}",
                    'source': 'fallback'
                }
            ])
        elif any(word in query_lower for word in ['technologie', 'intelligence artificielle', 'ia', 'informatique', 'digital']):
            results.extend([
                {
                    'title': f"Actualit√©s tech : {query}",
                    'url': "https://www.futura-sciences.com/tech/",
                    'snippet': f"Derni√®res actualit√©s et d√©couvertes sur {query}",
                    'source': 'fallback'
                },
                {
                    'title': f"Tech et innovation : {query}",
                    'url': "https://www.01net.com/",
                    'snippet': f"News et analyses tech sur {query}",
                    'source': 'fallback'
                }
            ])
        elif any(word in query_lower for word in ['science', 'recherche', '√©tude', 'd√©couverte']):
            results.extend([
                {
                    'title': f"Sciences et recherche : {query}",
                    'url': "https://www.futura-sciences.com/",
                    'snippet': f"Actualit√©s scientifiques et d√©couvertes sur {query}",
                    'source': 'fallback'
                },
                {
                    'title': f"Recherche scientifique : {query}",
                    'url': "https://www.cnrs.fr/",
                    'snippet': f"Travaux de recherche du CNRS concernant {query}",
                    'source': 'fallback'
                }
            ])
        else:
            # Fallback g√©n√©ral avec des sites fiables
            results.extend([
                {
                    'title': f"Encyclop√©die : {query}",
                    'url': "https://fr.wikipedia.org/",
                    'snippet': f"D√©finitions et informations encyclop√©diques sur {query}",
                    'source': 'fallback'
                },
                {
                    'title': f"Actualit√©s : {query}",
                    'url': "https://www.francetvinfo.fr/",
                    'snippet': f"Derni√®res actualit√©s concernant {query}",
                    'source': 'fallback'
                },
                {
                    'title': f"Culture et soci√©t√© : {query}",
                    'url': "https://www.radiofrance.fr/",
                    'snippet': f"Analyses et d√©bats sur {query}",
                    'source': 'fallback'
                }
            ])
        
        # Ajouter des sources compl√©mentaires
        results.extend([
            {
                'title': f"Forum et discussions : {query}",
                'url': "https://www.reddit.com/",
                'snippet': f"Discussions et avis de la communaut√© sur {query}",
                'source': 'fallback'
            },
            {
                'title': f"Ressources acad√©miques : {query}",
                'url': "https://scholar.google.com/",
                'snippet': f"Publications et articles acad√©miques sur {query}",
                'source': 'fallback'
            }
        ])
        
        logger.info(f"üîÑ {len(results)} r√©sultats de fallback cr√©√©s")
        return results[:5]  # Limiter √† 5 r√©sultats
    
    def search_web(self, query: str, max_results: int = None, enabled_engines: List[str] = None) -> List[Dict]:
        """Recherche web avec multiple APIs et fallbacks configurables"""
        if max_results is None:
            max_results = self.config.MAX_SEARCH_RESULTS
        
        if enabled_engines is None:
            enabled_engines = ["SerpApi", "SearXNG", "Serper", "Google-HTML", "DuckDuckGo-HTML"]
        
        logger.info(f"üöÄ D√©but recherche pour: '{query}'")
        
        # Nettoyer la requ√™te
        clean_query = query.strip()
        if len(clean_query) > 80:
            words = clean_query.split()
            clean_query = " ".join(words[-4:])  # 4 derniers mots
            logger.info(f"üìù Requ√™te simplifi√©e: '{clean_query}'")
        
        results = []
        
        # Recherche configurable selon les moteurs s√©lectionn√©s
        for i, engine in enumerate(enabled_engines, 1):
            # Arr√™ter si on a assez de r√©sultats
            if len(results) >= max_results:
                break
                
            logger.info(f"üéØ √âtape {i}: Tentative {engine}")
            engine_results = []
            
            if engine == "SerpApi":
                engine_results = self.search_serpapi_free(clean_query, max_results)
            elif engine == "Serper" and self.config.SERPER_API_KEY:
                engine_results = self.search_serper(clean_query, max_results)
            elif engine == "SearXNG":
                engine_results = self.search_searxng(clean_query, max_results)
            elif engine == "Google-HTML":
                engine_results = self.search_html_google(clean_query, max_results)
            elif engine == "Bing-HTML":
                engine_results = self.search_html_bing(clean_query, max_results)
            elif engine == "Startpage-HTML":
                engine_results = self.search_html_startpage(clean_query, max_results)
            elif engine == "DuckDuckGo-HTML":
                engine_results = self.search_html_duckduckgo(clean_query, max_results)
            else:
                logger.warning(f"‚ö†Ô∏è Moteur {engine} non support√© ou non configur√©")
                continue
            
            if engine_results:
                logger.info(f"‚úÖ {engine}: {len(engine_results)} r√©sultats trouv√©s")
                results.extend(engine_results)
                
                # Arr√™t anticip√© si on a suffisamment de r√©sultats de qualit√©
                if len(engine_results) >= max_results // 2:
                    logger.info(f"‚úÖ {engine} suffisant: {len(engine_results)} r√©sultats - arr√™t anticip√©")
                    break
            else:
                logger.warning(f"‚ö†Ô∏è {engine}: aucun r√©sultat trouv√©")
            
            # D√©lai entre les moteurs
            time.sleep(0.5 if engine == "SerpApi" else 1)
        
        # Utiliser fallback seulement si vraiment aucun r√©sultat
        if len(results) == 0:
            logger.warning("‚ö†Ô∏è Aucun r√©sultat trouv√© avec tous les moteurs, utilisation du fallback")
            results = self.create_fallback_results(clean_query)
        elif len(results) < 3:
            # Ajouter quelques r√©sultats de fallback si on en a tr√®s peu
            logger.info(f"‚ÑπÔ∏è Seulement {len(results)} r√©sultats trouv√©s, ajout de fallbacks compl√©mentaires")
            fallback_results = self.create_fallback_results(clean_query)[:2]  # Max 2 fallbacks
            results.extend(fallback_results)
        
        # Nettoyer et d√©dupliquer
        seen_urls = set()
        unique_results = []
        
        for result in results:
            url = result.get('url', '')
            if url and url not in seen_urls and url.startswith('http'):
                seen_urls.add(url)
                unique_results.append(result)
        
        final_results = unique_results[:max_results]
        
        logger.info(f"‚úÖ Recherche termin√©e: {len(final_results)} r√©sultats finaux")
        for i, result in enumerate(final_results, 1):
            logger.info(f"   {i}. {result['title'][:50]}... [{result['source']}]")
        
        return final_results 