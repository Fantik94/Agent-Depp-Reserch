import streamlit as st
import logging
import time
from research_agent import ResearchAgent
from config import Config
from link_ranker import display_ranked_links
import json
import threading
from datetime import datetime
import pandas as pd
from typing import List

# Configuration du logging avec plus de dÃ©tails
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Configuration Streamlit
st.set_page_config(
    page_title="Agent de Recherche",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personnalisÃ© amÃ©liorÃ©
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 15px;
        margin-bottom: 2rem;
        color: white;
        box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }
    
    .search-box {
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        padding: 2rem;
        border-radius: 15px;
        border: 1px solid #dee2e6;
        margin: 1rem 0;
        box-shadow: 0 2px 10px rgba(0,0,0,0.05);
    }
    
    .result-box {
        background: white;
        padding: 2rem;
        border-radius: 15px;
        box-shadow: 0 4px 20px rgba(0,0,0,0.1);
        margin: 1rem 0;
        border-left: 5px solid #28a745;
    }
    
    .step-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        margin: 1rem 0;
    }
    
    .step-box {
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        padding: 1.5rem;
        border-radius: 12px;
        margin: 0.5rem 0;
        border-left: 4px solid #6c757d;
        transition: all 0.3s ease;
        position: relative;
    }
    
    .step-waiting {
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        border-left: 4px solid #6c757d;
        opacity: 0.6;
    }
    
    .step-active {
        background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
        border-left: 4px solid #2196f3;
        box-shadow: 0 4px 15px rgba(33, 150, 243, 0.3);
        animation: pulse 2s ease-in-out infinite alternate;
    }
    
    .step-completed {
        background: linear-gradient(135deg, #e8f5e8 0%, #c8e6c9 100%);
        border-left: 4px solid #4caf50;
        box-shadow: 0 4px 15px rgba(76, 175, 80, 0.2);
    }
    
    .step-error {
        background: linear-gradient(135deg, #ffebee 0%, #ffcdd2 100%);
        border-left: 4px solid #f44336;
        box-shadow: 0 4px 15px rgba(244, 67, 54, 0.2);
    }
    
    .step-title {
        font-size: 1.1rem;
        font-weight: 600;
        margin-bottom: 0.5rem;
        display: flex;
        align-items: center;
        gap: 0.5rem;
    }
    
    .step-details {
        font-size: 0.9rem;
        color: #6c757d;
        margin-top: 0.5rem;
    }
    
    .step-time {
        position: absolute;
        top: 0.5rem;
        right: 1rem;
        font-size: 0.8rem;
        color: #6c757d;
    }
    
    .log-container {
        background: #f8f9fa;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
        max-height: 300px;
        overflow-y: auto;
        font-family: 'Courier New', monospace;
        font-size: 0.85rem;
    }
    
    .source-item {
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 10px;
        border-left: 4px solid #17a2b8;
        transition: transform 0.2s ease;
    }
    
    .source-item:hover {
        transform: translateX(5px);
    }
    
    @keyframes pulse {
        from { 
            box-shadow: 0 4px 15px rgba(33, 150, 243, 0.3);
        }
        to { 
            box-shadow: 0 4px 25px rgba(33, 150, 243, 0.6);
        }
    }
    
    .metric-card {
        background: white;
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
</style>
""", unsafe_allow_html=True)

def init_session_state():
    """Initialise l'Ã©tat de session"""
    if 'search_history' not in st.session_state:
        st.session_state.search_history = []
    if 'last_result' not in st.session_state:
        st.session_state.last_result = None
    if 'search_running' not in st.session_state:
        st.session_state.search_running = False
    if 'search_steps' not in st.session_state:
        st.session_state.search_steps = {}
    if 'search_logs' not in st.session_state:
        st.session_state.search_logs = []
    if 'search_start_time' not in st.session_state:
        st.session_state.search_start_time = None

def get_agent(llm_provider: str, search_engines: List[str], scraping_method: str):
    """Obtient un agent configurÃ© selon les paramÃ¨tres"""
    # ClÃ© pour identifier la configuration
    config_key = f"{llm_provider}_{'-'.join(search_engines)}_{scraping_method}"
    
    # VÃ©rifier si on a dÃ©jÃ  un agent avec cette configuration
    if 'agent_config' not in st.session_state or st.session_state.agent_config != config_key:
        # CrÃ©er un nouvel agent avec les bons paramÃ¨tres
        st.session_state.agent = ResearchAgent(
            llm_provider=llm_provider,
            search_engines=search_engines,
            scraping_method=scraping_method
        )
        st.session_state.agent_config = config_key
        logger.info(f"ğŸ”§ Agent reconfigurÃ©: LLM={llm_provider}, Moteurs={search_engines}")
    
    return st.session_state.agent

def display_header():
    """Affiche l'en-tÃªte de l'application"""
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ” Agent de Recherche Intelligent</h1>
        <p>Recherche intelligente avec analyse multi-sources et synthÃ¨se IA</p>
    </div>
    """, unsafe_allow_html=True)

def display_search_interface():
    """Affiche l'interface de recherche"""
    st.markdown('<div class="search-box">', unsafe_allow_html=True)
    
    # Input principal
    user_query = st.text_input(
        "ğŸ” Votre question de recherche :",
        placeholder="Ex: Intelligence artificielle avantages et inconvÃ©nients",
        help="Posez une question claire et prÃ©cise pour obtenir les meilleurs rÃ©sultats"
    )
    
    # Options avancÃ©es
    with st.expander("âš™ï¸ Options avancÃ©es"):
        col_opt1, col_opt2 = st.columns(2)
        
        with col_opt1:
            deep_search = st.checkbox(
                "ğŸ”¬ Recherche approfondie", 
                value=False,
                help="GÃ©nÃ¨re plus de requÃªtes et analyse plus d'articles (plus lent mais plus complet)"
            )
        
        with col_opt2:
            max_articles = st.slider(
                "ğŸ“° Nombre max d'articles", 
                min_value=3, 
                max_value=15, 
                value=5,
                help="Nombre maximum d'articles Ã  analyser en dÃ©tail"
            )
        
        # Configuration moteurs de recherche et scraping
        col_llm, col_search = st.columns(2)
        
        with col_llm:
            # SÃ©lecteur de modÃ¨le LLM
            llm_provider = st.selectbox(
                "ğŸ¤– ModÃ¨le LLM",
                ["groq", "mistral", "ollama"],
                index=0,  # Groq par dÃ©faut
                help="Groq: Gratuit et rapide | Mistral: Payant mais puissant | Ollama: Local et gratuit"
            )
            
            # Informations sur le modÃ¨le sÃ©lectionnÃ©
            if llm_provider == "groq":
                st.info("ğŸš€ **Groq**: ModÃ¨le Llama gratuit et trÃ¨s rapide")
            elif llm_provider == "mistral":
                st.warning("ğŸ’³ **Mistral**: ModÃ¨le puissant mais payant")
            elif llm_provider == "ollama":
                st.info("ğŸ  **Ollama**: ModÃ¨le local gratuit")
        
        with col_search:
            # SÃ©lecteur de moteurs de recherche
            search_engines = st.multiselect(
                "ğŸ” Moteurs de recherche",
                ["SerpApi", "Serper", "SearXNG", "Google-HTML", "Bing-HTML", "DuckDuckGo-HTML", "Startpage-HTML"],
                default=["SerpApi", "Google-HTML"],
                help="Choisissez les moteurs Ã  utiliser (dans l'ordre). HTML contourne les limitations."
            )
            
            # MÃ©thode de scraping
            scraping_method = st.selectbox(
                "ğŸ“° MÃ©thode de scraping",
                ["newspaper", "beautifulsoup", "both"],
                index=2,  # Both par dÃ©faut
                help="Newspaper: Plus rapide | BeautifulSoup: Plus robuste | Both: Les deux"
            )
    
    # Boutons et options
    col1, col2, col3, col4 = st.columns([2, 1, 1, 2])
    
    with col1:
        search_button = st.button("ğŸš€ Lancer la recherche", type="primary", use_container_width=True)
    
    with col2:
        if st.button("ğŸ—‘ï¸ Effacer", use_container_width=True):
            # ArrÃªter la recherche en cours
            st.session_state.search_running = False
            
            # Nettoyer complÃ¨tement l'interface
            st.session_state.last_result = None
            st.session_state.search_steps = {}
            st.session_state.search_logs = []
            
            # Nettoyer l'historique si souhaitÃ©
            # st.session_state.search_history = []
            
            # Afficher un message de confirmation
            st.success("ğŸ§¹ Interface nettoyÃ©e ! Recherche arrÃªtÃ©e.")
            st.rerun()
    
    with col3:
        show_logs = st.checkbox("ğŸ“‹ Logs", value=False)
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    return user_query, search_button, show_logs, deep_search, max_articles, llm_provider, search_engines, scraping_method

def add_search_log(message: str, level: str = "info"):
    """Ajoute un log Ã  la liste des logs de recherche"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    log_entry = {
        "time": timestamp,
        "message": message,
        "level": level
    }
    st.session_state.search_logs.append(log_entry)
    
    # Garder seulement les 50 derniers logs
    if len(st.session_state.search_logs) > 50:
        st.session_state.search_logs = st.session_state.search_logs[-50:]

def update_search_step(step_id: str, status: str, title: str, details: str = "", duration: str = ""):
    """Met Ã  jour une Ã©tape de recherche"""
    st.session_state.search_steps[step_id] = {
        "status": status,
        "title": title,
        "details": details,
        "duration": duration,
        "timestamp": datetime.now().strftime("%H:%M:%S")
    }
    
    # Ajouter au log
    if status == "active":
        add_search_log(f"ğŸ”„ {title} - {details}", "info")
    elif status == "completed":
        add_search_log(f"âœ… {title} terminÃ© - {details}", "success")
    elif status == "error":
        add_search_log(f"âŒ {title} Ã©chouÃ© - {details}", "error")

def display_search_progress():
    """Affiche le progrÃ¨s de la recherche en temps rÃ©el"""
    if not st.session_state.search_steps:
        return
    
    st.markdown("### ğŸš€ Progression de la recherche")
    
    steps_config = [
        {"id": "plan", "icon": "ğŸ“‹", "default_title": "GÃ©nÃ©ration du plan"},
        {"id": "search", "icon": "ğŸ”", "default_title": "Recherche web"},
        {"id": "scraping", "icon": "ğŸ“°", "default_title": "Analyse des articles"},
        {"id": "synthesis", "icon": "âœï¸", "default_title": "SynthÃ¨se finale"}
    ]
    
    # Afficher les Ã©tapes en grille 2x2
    st.markdown('<div class="step-container">', unsafe_allow_html=True)
    
    for i, step_config in enumerate(steps_config):
        step_id = step_config["id"]
        step_data = st.session_state.search_steps.get(step_id, {})
        
        status = step_data.get("status", "waiting")
        title = step_data.get("title", step_config["default_title"])
        details = step_data.get("details", "")
        duration = step_data.get("duration", "")
        timestamp = step_data.get("timestamp", "")
        
        # DÃ©finir la classe CSS selon le statut
        if status == "active":
            step_class = "step-active"
            icon = "â³"
        elif status == "completed":
            step_class = "step-completed"
            icon = "âœ…"
        elif status == "error":
            step_class = "step-error"
            icon = "âŒ"
        else:
            step_class = "step-waiting"
            icon = "â¸ï¸"
        
        # Afficher l'Ã©tape
        st.markdown(f"""
        <div class="{step_class} step-box">
            <div class="step-time">{timestamp}</div>
            <div class="step-title">
                {icon} {step_config['icon']} {title}
            </div>
            <div class="step-details">
                {details}
                {f"<br><small>â±ï¸ {duration}</small>" if duration else ""}
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown('</div>', unsafe_allow_html=True)

def display_logs(show_logs: bool):
    """Affiche les logs de recherche"""
    if show_logs and st.session_state.search_logs:
        st.markdown("### ğŸ“‹ Logs de recherche")
        
        # Conteneur de logs
        log_html = '<div class="log-container">'
        
        for log in st.session_state.search_logs[-20:]:  # 20 derniers logs
            color = {
                "info": "#17a2b8",
                "success": "#28a745", 
                "error": "#dc3545",
                "warning": "#ffc107"
            }.get(log["level"], "#6c757d")
            
            log_html += f"""
            <div style="margin: 0.2rem 0; color: {color};">
                <span style="color: #6c757d;">[{log['time']}]</span> {log['message']}
            </div>
            """
        
        log_html += '</div>'
        st.markdown(log_html, unsafe_allow_html=True)

class SearchProgressTracker:
    """Classe pour suivre le progrÃ¨s de recherche"""
    
    def __init__(self):
        self.start_time = time.time()
        st.session_state.search_start_time = self.start_time
        st.session_state.search_logs = []
        st.session_state.search_steps = {}
        
    def get_duration(self, start_time):
        """Calcule la durÃ©e Ã©coulÃ©e"""
        duration = time.time() - start_time
        return f"{duration:.1f}s"
    
    def start_step(self, step_id: str, title: str, details: str = ""):
        """DÃ©marre une Ã©tape"""
        self.step_start_time = time.time()
        update_search_step(step_id, "active", title, details)
        
    def complete_step(self, step_id: str, title: str, details: str = ""):
        """Termine une Ã©tape"""
        duration = self.get_duration(self.step_start_time)
        update_search_step(step_id, "completed", title, details, duration)
        
    def error_step(self, step_id: str, title: str, details: str = ""):
        """Marque une Ã©tape comme Ã©chouÃ©e"""
        duration = self.get_duration(self.step_start_time)
        update_search_step(step_id, "error", title, details, duration)

def research_with_progress_tracking(agent, query, deep_search=False, max_articles=5, search_engines=None, scraping_method="both"):
    """Effectue la recherche avec suivi du progrÃ¨s"""
    tracker = SearchProgressTracker()
    
    if search_engines is None:
        search_engines = ["SerpApi", "SearXNG"]
    
    try:
        # VÃ©rifier si la recherche doit continuer
        if not st.session_state.get('search_running', True):
            logger.info("ğŸ›‘ Recherche interrompue par l'utilisateur")
            return None
        # Ã‰tape 1: GÃ©nÃ©ration du plan
        tracker.start_step("plan", "GÃ©nÃ©ration du plan", "Analyse de votre question avec Mistral AI")
        
        # GÃ©nÃ©rer un plan plus dÃ©taillÃ© en mode approfondi
        if deep_search:
            plan = agent.llm_client.generate_deep_search_plan(query)
        else:
            plan = agent.llm_client.generate_search_plan(query)
            
        queries_count = len(plan.get("requetes_recherche", []))
        mode_text = "approfondie" if deep_search else "standard"
        tracker.complete_step("plan", "Plan gÃ©nÃ©rÃ©", f"{queries_count} requÃªtes de recherche crÃ©Ã©es (mode {mode_text})")
        
        # Ã‰tape 2: Recherche web
        tracker.start_step("search", "Recherche web", f"ExÃ©cution de {queries_count} requÃªtes de recherche")
        all_search_results = []
        
        for i, search_query in enumerate(plan.get("requetes_recherche", [query]), 1):
            # VÃ©rifier si la recherche doit continuer
            if not st.session_state.get('search_running', True):
                logger.info("ğŸ›‘ Recherche interrompue pendant la recherche web")
                return None
                
            add_search_log(f"ğŸ” RequÃªte {i}/{queries_count}: {search_query}")
            results = agent.search_api.search_web(search_query, enabled_engines=search_engines)
            all_search_results.extend(results)
            
            # Mise Ã  jour du progrÃ¨s
            progress_details = f"RequÃªte {i}/{queries_count} - {len(results)} rÃ©sultats trouvÃ©s"
            update_search_step("search", "active", "Recherche web", progress_details)
        
        # Supprimer les doublons
        unique_results = []
        seen_urls = set()
        for result in all_search_results:
            if result['url'] not in seen_urls:
                seen_urls.add(result['url'])
                unique_results.append(result)
        
        tracker.complete_step("search", "Recherche terminÃ©e", f"{len(unique_results)} rÃ©sultats uniques trouvÃ©s")
        
        # Ã‰tape 3: Scraping
        # VÃ©rifier si la recherche doit continuer
        if not st.session_state.get('search_running', True):
            logger.info("ğŸ›‘ Recherche interrompue avant le scraping")
            return None
            
        tracker.start_step("scraping", "Analyse des articles", f"Extraction du contenu de {len(unique_results)} sources")
        
        # Filtrer les rÃ©sultats fallback pour le scraping
        scrapable_results = [result for result in unique_results if result['source'] != 'fallback']
        fallback_count = len(unique_results) - len(scrapable_results)
        
        if fallback_count > 0:
            logger.info(f"âš ï¸ {fallback_count} rÃ©sultats de fallback ignorÃ©s pour le scraping")
            add_search_log(f"âš ï¸ {fallback_count} rÃ©sultats gÃ©nÃ©riques ignorÃ©s (Ã©vite les erreurs)")
        
        if scrapable_results:
            urls_to_scrape = [result['url'] for result in scrapable_results[:max_articles * 2]]
            scraped_articles = agent.scraper.scrape_multiple_urls(urls_to_scrape, max_articles=max_articles, method=scraping_method)
        else:
            logger.warning("âš ï¸ Aucune URL scrapable trouvÃ©e, utilisation de la synthÃ¨se basique")
            add_search_log("âš ï¸ Aucun article Ã  scraper - synthÃ¨se basÃ©e sur les snippets seulement")
            scraped_articles = []
        
        tracker.complete_step("scraping", "Articles analysÃ©s", f"{len(scraped_articles)} articles extraits avec succÃ¨s")
        
        # Ã‰tape 4: SynthÃ¨se
        # VÃ©rifier si la recherche doit continuer
        if not st.session_state.get('search_running', True):
            logger.info("ğŸ›‘ Recherche interrompue avant la synthÃ¨se")
            return None
            
        tracker.start_step("synthesis", "SynthÃ¨se finale", "GÃ©nÃ©ration de la rÃ©ponse avec Mistral AI")
        synthesis = agent.llm_client.synthesize_results(query, unique_results, scraped_articles)
        
        total_duration = tracker.get_duration(tracker.start_time)
        tracker.complete_step("synthesis", "SynthÃ¨se terminÃ©e", f"Recherche complÃ¨te en {total_duration}")
        
        return {
            "user_query": query,  # Nom cohÃ©rent pour le classement
            "query": query,
            "plan": plan,
            "search_results": unique_results,
            "scraped_articles": scraped_articles,
            "synthesis": synthesis,
            "stats": {
                "search_results_count": len(unique_results),
                "scraped_articles_count": len(scraped_articles),
                "search_queries_used": queries_count,
                "total_duration": total_duration
            }
        }
        
    except Exception as e:
        tracker.error_step("synthesis", "Erreur", str(e))
        raise e

def display_results(result):
    """Affiche les rÃ©sultats de recherche"""
    if not result:
        return
    
    # SynthÃ¨se principale
    st.markdown('<div class="result-box">', unsafe_allow_html=True)
    st.markdown("## ğŸ“ SynthÃ¨se des rÃ©sultats")
    st.markdown(result['synthesis'])
    st.markdown('</div>', unsafe_allow_html=True)
    
    # Statistiques en cartes
    st.markdown("### ğŸ“Š Statistiques de la recherche")
    stats = result['stats']
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown(f"""
        <div class="metric-card">
            <h3 style="color: #2196f3; margin: 0;">ğŸ”</h3>
            <h2 style="margin: 0.5rem 0;">{stats['search_results_count']}</h2>
            <p style="margin: 0; color: #6c757d;">Sources trouvÃ©es</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="metric-card">
            <h3 style="color: #4caf50; margin: 0;">ğŸ“°</h3>
            <h2 style="margin: 0.5rem 0;">{stats['scraped_articles_count']}</h2>
            <p style="margin: 0; color: #6c757d;">Articles analysÃ©s</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown(f"""
        <div class="metric-card">
            <h3 style="color: #ff9800; margin: 0;">ğŸ¯</h3>
            <h2 style="margin: 0.5rem 0;">{stats['search_queries_used']}</h2>
            <p style="margin: 0; color: #6c757d;">RequÃªtes utilisÃ©es</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        st.markdown(f"""
        <div class="metric-card">
            <h3 style="color: #9c27b0; margin: 0;">â±ï¸</h3>
            <h2 style="margin: 0.5rem 0;">{stats.get('total_duration', 'N/A')}</h2>
            <p style="margin: 0; color: #6c757d;">DurÃ©e totale</p>
        </div>
        """, unsafe_allow_html=True)
    
    # Plan de recherche
    with st.expander("ğŸ—‚ï¸ Plan de recherche utilisÃ©"):
        plan = result['plan']
        st.write("**StratÃ©gie :**", plan.get('strategie', 'Non disponible'))
        
        if 'requetes_recherche' in plan:
            st.write("**RequÃªtes exÃ©cutÃ©es :**")
            for i, query in enumerate(plan['requetes_recherche'], 1):
                st.write(f"{i}. `{query}`")
    
    # Sources consultÃ©es
    with st.expander(f"ğŸ“š Sources consultÃ©es ({len(result['search_results'])})"):
        for i, source in enumerate(result['search_results'][:15], 1):
            st.markdown(f"""
            <div class="source-item">
                <strong>{i}. {source['title']}</strong><br>
                <small style="color: #6c757d;">{source['url']}</small><br>
                <span style="background: #e3f2fd; padding: 0.2rem 0.5rem; border-radius: 4px; font-size: 0.8rem;">
                    {source['source']}
                </span><br>
                <div style="margin-top: 0.5rem;">
                    {source['snippet'][:250]}...
                </div>
            </div>
            """, unsafe_allow_html=True)
    
    # Articles analysÃ©s
    if result['scraped_articles']:
        with st.expander(f"ğŸ“° Articles analysÃ©s en dÃ©tail ({len(result['scraped_articles'])})"):
            for i, article in enumerate(result['scraped_articles'], 1):
                st.markdown(f"**{i}. {article['title']}**")
                st.write(f"ğŸ”— Source: {article['url']}")
                if article.get('publish_date'):
                    st.write(f"ğŸ“… Date: {article['publish_date']}")
                st.write(f"ğŸ“„ Extrait: {article['content'][:400]}...")
                st.markdown("---")
    
    # Nouveaux tableaux et analyses dÃ©taillÃ©es
    st.markdown("---")
    
    # Onglets pour organiser les analyses dÃ©taillÃ©es
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ”— Liens classÃ©s", "ğŸ“Š Tableau dÃ©taillÃ©", "ğŸ”¬ Analyse approfondie", "ğŸ“ˆ MÃ©triques avancÃ©es"])
    
    with tab1:
        display_ranked_links(result)
    
    with tab2:
        display_detailed_results_table(result)
    
    with tab3:
        display_research_insights(result)
    
    with tab4:
        display_advanced_metrics(result)

def display_sidebar():
    """Affiche la barre latÃ©rale"""
    with st.sidebar:
        st.markdown("## âš™ï¸ Configuration")
        
        config = Config()
        st.write(f"**ğŸ¤– ModÃ¨le LLM :** {config.MISTRAL_MODEL}")
        st.write(f"**ğŸ” Max rÃ©sultats :** {config.MAX_SEARCH_RESULTS}")
        st.write(f"**ğŸ“° Max articles :** {config.MAX_SCRAPED_ARTICLES}")
        
        # Status des API
        st.markdown("### ğŸ”Œ Status des APIs")
        
        # LLM APIs
        if config.MISTRAL_API_KEY:
            st.success("âœ… Mistral AI")
        else:
            st.error("âŒ Mistral AI")
        
        # Moteurs de recherche
        st.markdown("**ğŸ” Moteurs de recherche :**")
        if config.SERP_API_KEY:
            st.success("âœ… SerpApi (clÃ© personnelle)")
        else:
            st.error("âŒ SerpApi")
        
        if config.SERPER_API_KEY:
            st.success("âœ… Serper.dev")
        else:
            st.info("â„¹ï¸ Serper.dev non configurÃ©")
        
        st.success("âœ… SearXNG (gratuit)")
        st.success("âœ… Google HTML (gratuit)")
        st.success("âœ… Bing HTML (gratuit)")
        st.success("âœ… DuckDuckGo HTML (gratuit)")
        st.success("âœ… Startpage HTML (gratuit)")
        
        # MÃ©thodes de scraping
        st.markdown("**ğŸ“° Scraping disponible :**")
        st.success("âœ… Newspaper3k")
        st.success("âœ… BeautifulSoup")
        
        # Historique
        st.markdown("### ğŸ“œ Historique")
        if st.session_state.search_history:
            for i, query in enumerate(reversed(st.session_state.search_history[-5:]), 1):
                if st.button(f"{i}. {query[:25]}...", key=f"history_{i}"):
                    st.session_state.selected_query = query
        else:
            st.write("Aucune recherche")

def display_ranked_links(result):
    """Affiche les liens classÃ©s par pertinence avec dÃ©tails"""
    st.markdown("### ğŸ”— Liens trouvÃ©s classÃ©s par pertinence")
    
    if not result.get('search_results'):
        st.warning("Aucun lien trouvÃ©")
        return
    
    # Classifier les liens par pertinence
    ranked_links = rank_links_by_relevance(result['search_results'], result.get('user_query', ''))
    
    # Afficher les statistiques
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ğŸ”— Total liens", len(ranked_links))
    with col2:
        e_commerce_count = sum(1 for link in ranked_links if is_ecommerce_link(link['url']))
        st.metric("ğŸ›’ Sites e-commerce", e_commerce_count)
    with col3:
        amazon_count = sum(1 for link in ranked_links if 'amazon' in link['url'].lower())
        st.metric("ğŸ“¦ Amazon", amazon_count)
    
    # Affichage des liens classÃ©s
    st.markdown("#### ğŸ† Classement des rÃ©sultats (du meilleur au moins bon)")
    
    for i, link in enumerate(ranked_links, 1):
        # Calculer le score de pertinence
        relevance_score = calculate_relevance_score(link, result.get('user_query', ''))
        
        # DÃ©terminer les badges
        badges = get_link_badges(link['url'])
        if badges is None:
            badges = []
        
        # CrÃ©er une carte pour chaque lien
        with st.container():
            # En-tÃªte avec rang et score
            col_rank, col_content = st.columns([1, 9])
            
            with col_rank:
                # MÃ©daille pour les 3 premiers
                if i == 1:
                    st.markdown("# ğŸ¥‡")
                elif i == 2:
                    st.markdown("# ğŸ¥ˆ")
                elif i == 3:
                    st.markdown("# ğŸ¥‰")
                else:
                    st.markdown(f"## #{i}")
            
            with col_content:
                # Titre avec lien cliquable
                st.markdown(f"**[{link['title']}]({link['url']})**")
                
                # Badges
                if badges:  # VÃ©rifier que badges n'est pas vide
                    badge_html = " ".join([f'<span style="background: {color}; color: white; padding: 2px 6px; border-radius: 10px; font-size: 10px; margin-right: 4px;">{text}</span>' 
                                         for text, color in badges])
                    if badge_html:
                        st.markdown(badge_html, unsafe_allow_html=True)
                
                # Description/snippet
                if link.get('snippet'):
                    st.write(f"ğŸ“ {link['snippet'][:200]}{'...' if len(link['snippet']) > 200 else ''}")
                
                # Informations techniques
                col_tech1, col_tech2, col_tech3 = st.columns(3)
                with col_tech1:
                    st.caption(f"â­ Score: {relevance_score:.1f}/10")
                with col_tech2:
                    domain = extract_domain(link['url'])
                    st.caption(f"ğŸŒ {domain}")
                with col_tech3:
                    st.caption(f"ğŸ” Via {link.get('source', 'N/A')}")
        
                    st.markdown("---")
     
    # Actions sur les liens
    st.markdown("#### ğŸ’¾ Actions")
    col_action1, col_action2, col_action3 = st.columns(3)
     
    with col_action1:
        if st.button("ğŸ“‹ Copier le top 5", help="Copier les 5 meilleurs liens"):
            top_5_text = create_top_links_text(ranked_links[:5])
            st.code(top_5_text)
     
    with col_action2:
        if st.button("ğŸ“„ Exporter CSV", help="TÃ©lÃ©charger au format CSV"):
            csv_data = create_csv_export(ranked_links)
            st.download_button(
                label="ğŸ’¾ TÃ©lÃ©charger",
                data=csv_data,
                file_name="liens_classes.csv",
                mime="text/csv"
            )
     
    with col_action3:
        if st.button("ğŸ”— URLs uniquement", help="Liste simple des URLs"):
            urls_text = "\n".join([link['url'] for link in ranked_links[:10]])
            st.code(urls_text)
     
    # Section filtres
    with st.expander("ğŸ”§ Filtrer les rÃ©sultats"):
        col_filter1, col_filter2 = st.columns(2)
        
        with col_filter1:
            # Filtre par type de site
            site_types = st.multiselect(
                "Type de site",
                ["E-commerce", "Blog", "News", "Forum", "Officiel"],
                help="Filtrer par type de site"
            )
        
        with col_filter2:
            # Filtre par domaine
            domains = list(set([extract_domain(link['url']) for link in ranked_links]))
            selected_domains = st.multiselect(
                "Domaines",
                domains[:10],  # Top 10 domaines
                help="Filtrer par domaine spÃ©cifique"
            )

def rank_links_by_relevance(links, query):
    """Classe les liens par pertinence"""
    scored_links = []
    
    for link in links:
        score = calculate_relevance_score(link, query)
        scored_links.append({**link, 'relevance_score': score})
    
    # Trier par score dÃ©croissant
    return sorted(scored_links, key=lambda x: x['relevance_score'], reverse=True)

def calculate_relevance_score(link, query):
    """Calcule un score de pertinence pour un lien"""
    score = 0
    query_words = query.lower().split()
    
    title = link.get('title', '').lower()
    snippet = link.get('snippet', '').lower()
    url = link.get('url', '').lower()
    
    # Points pour les mots de la requÃªte dans le titre (poids fort)
    for word in query_words:
        if word in title:
            score += 3
        if word in snippet:
            score += 2
        if word in url:
            score += 1
    
    # Bonus pour sites e-commerce si c'est une recherche produit
    if is_product_query(query) and is_ecommerce_link(url):
        score += 2
    
    # Bonus spÃ©cial Amazon
    if 'amazon' in url:
        score += 1.5
    
    # Bonus pour les sites populaires
    popular_domains = ['amazon.fr', 'amazon.com', 'leboncoin.fr', 'fnac.com', 'darty.com', 'boulanger.com']
    domain = extract_domain(url)
    if domain in popular_domains:
        score += 1
    
    # Malus pour les liens trop courts (peu d'info)
    if len(snippet) < 50:
        score -= 0.5
    
    return max(0, score)  # Score minimum 0

def is_product_query(query):
    """DÃ©tecte si c'est une recherche de produit"""
    product_keywords = ['meilleur', 'acheter', 'prix', 'pas cher', 'promo', 'solde', 'euro', 'â‚¬', 'test', 'avis', 'comparatif']
    return any(keyword in query.lower() for keyword in product_keywords)

def is_ecommerce_link(url):
    """DÃ©tecte si c'est un site e-commerce"""
    ecommerce_domains = ['amazon', 'fnac', 'darty', 'boulanger', 'cdiscount', 'leboncoin', 'rakuten', 'zalando', 'decathlon']
    return any(domain in url.lower() for domain in ecommerce_domains)

def extract_domain(url):
    """Extrait le domaine d'une URL"""
    try:
        from urllib.parse import urlparse
        return urlparse(url).netloc.replace('www.', '')
    except:
        return url.split('/')[2] if '/' in url else url

def get_link_badges(url):
    """Retourne les badges appropriÃ©s pour un lien"""
    badges = []
    url_lower = url.lower()
    
    if 'amazon' in url_lower:
        badges.append(("AMAZON", "#ff9900"))
    elif any(shop in url_lower for shop in ['fnac', 'darty', 'boulanger']):
        badges.append(("E-COMMERCE", "#007bff"))
    elif 'leboncoin' in url_lower:
        badges.append(("OCCASION", "#28a745"))
    
    if any(word in url_lower for word in ['test', 'review', 'avis']):
        badges.append(("AVIS", "#6f42c1"))
    
    if any(word in url_lower for word in ['promo', 'solde', 'reduction']):
        badges.append(("PROMO", "#dc3545"))
    
    return badges

def create_top_links_text(links):
    """CrÃ©e un texte formatÃ© avec les meilleurs liens"""
    text = "ğŸ† TOP LIENS TROUVÃ‰S\n" + "="*50 + "\n\n"
    
    for i, link in enumerate(links, 1):
        medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
        text += f"{medal} {link['title']}\n"
        text += f"   ğŸ”— {link['url']}\n"
        if link.get('snippet'):
            text += f"   ğŸ“ {link['snippet'][:100]}...\n"
        text += "\n"
    
    return text

def create_csv_export(links):
    """CrÃ©e un export CSV des liens"""
    import io
    import csv
    
    output = io.StringIO()
    writer = csv.writer(output)
    
    # En-tÃªtes
    writer.writerow(['Rang', 'Titre', 'URL', 'Domaine', 'Score', 'Snippet', 'Source'])
    
    # DonnÃ©es
    for i, link in enumerate(links, 1):
        writer.writerow([
            i,
            link['title'],
            link['url'],
            extract_domain(link['url']),
            f"{link.get('relevance_score', 0):.1f}",
            link.get('snippet', '')[:200],
            link.get('source', 'N/A')
        ])
    
    return output.getvalue()

def display_detailed_results_table(result):
    """Affiche un tableau dÃ©taillÃ© des rÃ©sultats de recherche"""
    st.markdown("### ğŸ“Š Tableau dÃ©taillÃ© des rÃ©sultats")
    
    # CrÃ©er les donnÃ©es pour le tableau
    table_data = []
    
    # Ajouter les sources de recherche
    for i, source in enumerate(result.get('search_results', []), 1):
        table_data.append({
            'NÂ°': i,
            'Type': 'ğŸ” Source',
            'Titre': source.get('title', 'N/A')[:60] + '...' if len(source.get('title', '')) > 60 else source.get('title', 'N/A'),
            'URL': source.get('url', 'N/A'),
            'Moteur': source.get('source', 'N/A'),
            'Snippet': source.get('snippet', 'N/A')[:100] + '...' if len(source.get('snippet', '')) > 100 else source.get('snippet', 'N/A'),
            'Status': 'âœ… TrouvÃ©'
        })
    
    # Ajouter les articles scrapÃ©s
    for i, article in enumerate(result.get('scraped_articles', []), 1):
        table_data.append({
            'NÂ°': len(result.get('search_results', [])) + i,
            'Type': 'ğŸ“° Article',
            'Titre': article.get('title', 'N/A')[:60] + '...' if len(article.get('title', '')) > 60 else article.get('title', 'N/A'),
            'URL': article.get('url', 'N/A'),
            'Moteur': 'Scraped',
            'Snippet': article.get('content', 'N/A')[:100] + '...' if len(article.get('content', '')) > 100 else article.get('content', 'N/A'),
            'Status': 'âœ… AnalysÃ©'
        })
    
    # CrÃ©er le DataFrame
    if table_data:
        df = pd.DataFrame(table_data)
        
        # Afficher le tableau avec style
        st.dataframe(
            df,
            use_container_width=True,
            hide_index=True,
            column_config={
                "NÂ°": st.column_config.NumberColumn("NÂ°", width="small"),
                "Type": st.column_config.TextColumn("Type", width="small"),
                "Titre": st.column_config.TextColumn("Titre", width="medium"),
                "URL": st.column_config.LinkColumn("URL", width="medium"),
                "Moteur": st.column_config.TextColumn("Moteur", width="small"),
                "Snippet": st.column_config.TextColumn("Extrait", width="large"),
                "Status": st.column_config.TextColumn("Status", width="small")
            }
        )
        
        # Statistiques
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ğŸ“Š Total rÃ©sultats", len(table_data))
        with col2:
            sources_count = len(result.get('search_results', []))
            st.metric("ğŸ” Sources trouvÃ©es", sources_count)
        with col3:
            articles_count = len(result.get('scraped_articles', []))
            st.metric("ğŸ“° Articles analysÃ©s", articles_count)
        with col4:
            if result.get('plan'):
                queries_count = len(result['plan'].get('requetes_recherche', []))
                st.metric("ğŸ¯ RequÃªtes exÃ©cutÃ©es", queries_count)
    else:
        st.warning("Aucun rÃ©sultat Ã  afficher dans le tableau")

def display_research_insights(result):
    """Affiche des insights dÃ©taillÃ©s sur la recherche"""
    st.markdown("### ğŸ”¬ Analyse approfondie de la recherche")
    
    # Plan de recherche dÃ©taillÃ©
    if result.get('plan'):
        plan = result['plan']
        with st.expander("ğŸ“‹ Plan de recherche gÃ©nÃ©rÃ©", expanded=True):
            st.markdown("**ğŸ¯ RequÃªtes de recherche :**")
            for i, query in enumerate(plan.get('requetes_recherche', []), 1):
                st.markdown(f"  {i}. `{query}`")
            
            if plan.get('types_sources'):
                st.markdown("**ğŸ“š Types de sources ciblÃ©es :**")
                for source_type in plan['types_sources']:
                    st.markdown(f"  â€¢ {source_type}")
            
            if plan.get('questions_secondaires'):
                st.markdown("**â“ Questions secondaires :**")
                for question in plan['questions_secondaires']:
                    st.markdown(f"  â€¢ {question}")
            
            if plan.get('strategie'):
                st.markdown(f"**ğŸ² StratÃ©gie :** {plan['strategie']}")
    
    # Analyse des sources
    if result.get('search_results'):
        with st.expander("ğŸ” Analyse des sources", expanded=True):
            sources = result['search_results']
            
            # RÃ©partition par moteur de recherche
            source_counts = {}
            for source in sources:
                engine = source.get('source', 'Inconnu')
                source_counts[engine] = source_counts.get(engine, 0) + 1
            
            st.markdown("**ğŸ“Š RÃ©partition par moteur de recherche :**")
            for engine, count in source_counts.items():
                percentage = (count / len(sources)) * 100
                st.markdown(f"  â€¢ {engine}: {count} rÃ©sultats ({percentage:.1f}%)")
            
            # Analyse des domaines
            domains = {}
            for source in sources:
                url = source.get('url', '')
                if url:
                    try:
                        domain = url.split('/')[2]
                        domains[domain] = domains.get(domain, 0) + 1
                    except:
                        pass
            
            if domains:
                st.markdown("**ğŸŒ Domaines les plus frÃ©quents :**")
                sorted_domains = sorted(domains.items(), key=lambda x: x[1], reverse=True)
                for domain, count in sorted_domains[:10]:
                    st.markdown(f"  â€¢ {domain}: {count} rÃ©sultat(s)")
    
    # QualitÃ© du scraping
    if result.get('scraped_articles'):
        with st.expander("ğŸ“° QualitÃ© du scraping", expanded=True):
            articles = result['scraped_articles']
            
            # Statistiques de longueur
            lengths = [len(article.get('content', '')) for article in articles]
            if lengths:
                avg_length = sum(lengths) / len(lengths)
                st.markdown(f"**ğŸ“ Longueur moyenne des articles :** {avg_length:.0f} caractÃ¨res")
                st.markdown(f"**ğŸ“ Article le plus long :** {max(lengths)} caractÃ¨res")
                st.markdown(f"**ğŸ“ Article le plus court :** {min(lengths)} caractÃ¨res")
            
            # Articles avec dates
            dated_articles = [a for a in articles if a.get('publish_date')]
            st.markdown(f"**ğŸ“… Articles avec date :** {len(dated_articles)}/{len(articles)}")
            
            # Articles avec auteurs
            authored_articles = [a for a in articles if a.get('authors')]
            st.markdown(f"**ğŸ‘¤ Articles avec auteur :** {len(authored_articles)}/{len(articles)}")

def display_advanced_metrics(result):
    """Affiche des mÃ©triques avancÃ©es sur la recherche"""
    st.markdown("### ğŸ“ˆ MÃ©triques avancÃ©es")
    
    # MÃ©triques de performance
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### âš¡ Performance")
        
        # Calcul du taux de succÃ¨s
        total_searches = len(result.get('search_results', []))
        successful_scrapes = len(result.get('scraped_articles', []))
        
        if total_searches > 0:
            success_rate = (successful_scrapes / total_searches) * 100
            st.metric("ğŸ¯ Taux de succÃ¨s du scraping", f"{success_rate:.1f}%")
        
        # Vitesse de recherche
        if result.get('stats', {}).get('total_duration'):
            duration_str = result['stats']['total_duration']
            try:
                duration_seconds = float(duration_str.replace('s', ''))
                speed = total_searches / duration_seconds if duration_seconds > 0 else 0
                st.metric("ğŸš€ Vitesse de recherche", f"{speed:.1f} rÃ©sultats/s")
            except:
                st.metric("ğŸš€ Vitesse de recherche", "N/A")
        
        # EfficacitÃ© du plan
        if result.get('plan'):
            queries_planned = len(result['plan'].get('requetes_recherche', []))
            queries_used = result.get('stats', {}).get('search_queries_used', 0)
            if queries_planned > 0:
                plan_efficiency = (queries_used / queries_planned) * 100
                st.metric("ğŸ“‹ EfficacitÃ© du plan", f"{plan_efficiency:.1f}%")
    
    with col2:
        st.markdown("#### ğŸ” QualitÃ© des donnÃ©es")
        
        # Richesse du contenu
        if result.get('scraped_articles'):
            articles = result['scraped_articles']
            
            # Longueur moyenne
            lengths = [len(article.get('content', '')) for article in articles]
            if lengths:
                avg_length = sum(lengths) / len(lengths)
                st.metric("ğŸ“ Longueur moyenne", f"{avg_length:.0f} chars")
            
            # DiversitÃ© des sources
            unique_domains = set()
            for article in articles:
                url = article.get('url', '')
                if url:
                    try:
                        domain = url.split('/')[2]
                        unique_domains.add(domain)
                    except:
                        pass
            
            if unique_domains:
                diversity = len(unique_domains) / len(articles) * 100
                st.metric("ğŸŒ DiversitÃ© des sources", f"{diversity:.1f}%")
            
            # FraÃ®cheur des donnÃ©es
            recent_articles = 0
            for article in articles:
                if article.get('publish_date'):
                    # Logique simple pour dÃ©terminer si c'est rÃ©cent
                    recent_articles += 1
            
            if articles:
                freshness = (recent_articles / len(articles)) * 100
                st.metric("ğŸ“… FraÃ®cheur des donnÃ©es", f"{freshness:.1f}%")
    
    # Graphique de rÃ©partition des sources
    if result.get('search_results'):
        st.markdown("#### ğŸ“Š RÃ©partition des sources par moteur")
        
        sources = result['search_results']
        source_counts = {}
        for source in sources:
            engine = source.get('source', 'Inconnu')
            source_counts[engine] = source_counts.get(engine, 0) + 1
        
        # CrÃ©er un DataFrame pour le graphique
        if source_counts:
            chart_data = pd.DataFrame(
                list(source_counts.items()),
                columns=['Moteur', 'Nombre']
            )
            
            # Graphique en barres
            st.bar_chart(chart_data.set_index('Moteur'))
            
            # Tableau de dÃ©tail
            st.markdown("**DÃ©tail par moteur :**")
            for engine, count in sorted(source_counts.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / len(sources)) * 100
                st.write(f"â€¢ **{engine}**: {count} rÃ©sultats ({percentage:.1f}%)")
    
    # Recommandations d'amÃ©lioration
    st.markdown("#### ğŸ’¡ Recommandations")
    
    recommendations = []
    
    # Analyse du taux de succÃ¨s
    if total_searches > 0:
        success_rate = (successful_scrapes / total_searches) * 100
        if success_rate < 50:
            recommendations.append("ğŸ”§ AmÃ©liorer la sÃ©lection des sources pour augmenter le taux de scraping")
        elif success_rate > 80:
            recommendations.append("âœ… Excellent taux de scraping ! Maintenir la qualitÃ©")
    
    # Analyse de la diversitÃ©
    if result.get('scraped_articles'):
        unique_domains = set()
        for article in result['scraped_articles']:
            url = article.get('url', '')
            if url:
                try:
                    domain = url.split('/')[2]
                    unique_domains.add(domain)
                except:
                    pass
        
        if len(unique_domains) < 3:
            recommendations.append("ğŸŒ Diversifier les sources pour obtenir des perspectives variÃ©es")
    
    # Analyse de la longueur
    if result.get('scraped_articles'):
        lengths = [len(article.get('content', '')) for article in result['scraped_articles']]
        if lengths and sum(lengths) / len(lengths) < 500:
            recommendations.append("ğŸ“ Chercher des articles plus dÃ©taillÃ©s pour une meilleure analyse")
    
    if recommendations:
        for rec in recommendations:
            st.write(rec)
    else:
        st.write("âœ… Excellente recherche ! Aucune amÃ©lioration majeure nÃ©cessaire.")

def main():
    """Fonction principale"""
    init_session_state()
    display_header()
    display_sidebar()
    
    # Interface de recherche
    user_query, search_button, show_logs, deep_search, max_articles, llm_provider, search_engines, scraping_method = display_search_interface()
    
    # Afficher le progrÃ¨s s'il y en a un
    if st.session_state.search_steps:
        display_search_progress()
        
        # Bouton d'arrÃªt pendant la recherche
        if st.session_state.get('search_running', False):
            if st.button("ğŸ›‘ ARRÃŠTER LA RECHERCHE", type="secondary", use_container_width=True):
                st.session_state.search_running = False
                add_search_log("ğŸ›‘ Recherche arrÃªtÃ©e par l'utilisateur", "warning")
                st.warning("ğŸ›‘ ArrÃªt de la recherche en cours...")
                st.rerun()
    
    # Afficher les logs en temps rÃ©el si recherche en cours
    if st.session_state.get('search_running', False) or show_logs:
        if st.session_state.search_logs:
            st.markdown("### ğŸ“‹ Logs en temps rÃ©el")
            
            # Conteneur de logs avec scroll automatique
            log_container = st.container()
            with log_container:
                # Afficher les 15 derniers logs
                recent_logs = st.session_state.search_logs[-15:] if len(st.session_state.search_logs) > 15 else st.session_state.search_logs
                
                for log in recent_logs:
                    # Couleurs selon le niveau et contenu
                    if "âœ…" in log["message"] or "rÃ©ussie" in log["message"]:
                        st.success(f"[{log['time']}] {log['message']}")
                    elif "ğŸ›‘" in log["message"] or "interrompue" in log["message"]:
                        st.warning(f"[{log['time']}] {log['message']}")
                    elif "âš ï¸" in log["message"] or "erreur" in log["message"]:
                        st.warning(f"[{log['time']}] {log['message']}")
                    elif "âŒ" in log["message"]:
                        st.error(f"[{log['time']}] {log['message']}")
                    elif "ğŸ”" in log["message"] or "ğŸš€" in log["message"]:
                        st.info(f"[{log['time']}] {log['message']}")
                    else:
                        st.text(f"[{log['time']}] {log['message']}")
                
                # Auto-scroll vers le bas
                if st.session_state.get('search_running', False):
                    st.markdown('<script>window.scrollTo(0, document.body.scrollHeight);</script>', unsafe_allow_html=True)
    
    # GÃ©rer la recherche
    if search_button and user_query:
        # Ajouter Ã  l'historique
        if user_query not in st.session_state.search_history:
            st.session_state.search_history.append(user_query)
        
        # Marquer le dÃ©but de la recherche
        st.session_state.search_running = True
        
        # Configurer le provider LLM sÃ©lectionnÃ©
        st.session_state.agent = get_agent(llm_provider, search_engines, scraping_method)
        
        # Afficher la configuration utilisÃ©e
        config_info = f"ğŸ¤– {llm_provider.upper()} | ğŸ” {', '.join(search_engines)} | ğŸ“° {scraping_method}"
        add_search_log(f"âš™ï¸ Configuration: {config_info}")
        
        # Placeholder pour les mises Ã  jour en temps rÃ©el
        progress_placeholder = st.empty()
        
        try:
            # Effectuer la recherche avec suivi
            with st.spinner("ğŸ”„ Recherche en cours..."):
                result = research_with_progress_tracking(
                    st.session_state.agent, 
                    user_query, 
                    deep_search=deep_search, 
                    max_articles=max_articles,
                    search_engines=search_engines,
                    scraping_method=scraping_method
                )
                
                # VÃ©rifier si la recherche a Ã©tÃ© interrompue
                if result is None:
                    st.warning("ğŸ›‘ Recherche interrompue par l'utilisateur")
                    st.session_state.search_running = False
                    return
                
                st.session_state.last_result = result
            
            st.success("âœ… Recherche terminÃ©e avec succÃ¨s !")
            st.session_state.search_running = False
            
        except Exception as e:
            st.session_state.search_running = False
            st.error(f"âŒ Erreur lors de la recherche: {str(e)}")
            logger.error(f"Erreur recherche: {e}")
            
            # Afficher les dÃ©tails de l'erreur
            with st.expander("ğŸ” DÃ©tails de l'erreur"):
                st.code(str(e))
                st.write("**Logs de debug :**")
                for log in st.session_state.search_logs[-10:]:
                    st.write(f"[{log['time']}] {log['message']}")
    
    # Afficher les rÃ©sultats
    if st.session_state.last_result:
        display_results(st.session_state.last_result)
    
    # Instructions
    if not st.session_state.last_result:
        st.markdown("""
        ### ğŸ’¡ Guide d'utilisation
        
        1. **Posez votre question** dans le champ ci-dessus
        2. **Cliquez sur "Lancer la recherche"** pour dÃ©marrer
        3. **Suivez le progrÃ¨s** en temps rÃ©el avec les Ã©tapes colorÃ©es
        4. **Explorez les rÃ©sultats** avec les sources et articles dÃ©taillÃ©s
        
        **âœ¨ Exemples de questions efficaces :**
        - `Intelligence artificielle avantages inconvÃ©nients`
        - `TÃ©lÃ©travail impact productivitÃ© 2024`
        - `Changement climatique solutions`
        - `JeÃ»ne intermittent effets santÃ©`
        
        **ğŸ”§ Activez les logs** pour voir les dÃ©tails techniques de la recherche.
        """)

if __name__ == "__main__":
    main() 